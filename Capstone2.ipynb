{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Data Science Capstone\n",
    "\n",
    "## Neighborhood similarity in major cities\n",
    "\n",
    "#### In completion of requirements for the IBM Data Science Professional Certificate on Coursera\n",
    "\n",
    "Daniel Nezich\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cities compared:\n",
    "* Canada\n",
    "    * Toronto\n",
    "    * Montréal\n",
    "    * Vancouver\n",
    "    * Halifax\n",
    "* United States of America\n",
    "    * New York City\n",
    "    * Boston\n",
    "    * Chicago\n",
    "    * San Fransisco\n",
    "* France\n",
    "    * Paris\n",
    "* England\n",
    "    * London\n",
    "\n",
    "First, get a list of Forward Sortation Areas as proxies for neighborhoods for cities in Canada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data directory\n",
    "DIR_DATA = 'Data/'\n",
    "DIR_RESULTS = 'Results/'\n",
    "\n",
    "# General\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Postal Codes\n",
    "import requests\n",
    "# from bs4 import BeautifulSoup # had to install to environment in Anaconda\n",
    "import lxml # had to install to environment in Anaconda, backdated to 4.6.1 (4.6.2 current) for pandas read_html()\n",
    "import html5lib # had to install to environment in Anaconda (1.1 current) for pandas read_html()\n",
    "\n",
    "# Geocoding\n",
    "import config\n",
    "from geopy.geocoders import Here\n",
    "from geopy import distance\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Census Features\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Venue Features\n",
    "import config\n",
    "import importlib\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# K-Means Clustering\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Optimal K-Means Model\n",
    "import folium\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib.colors\n",
    "from folium.features import DivIcon\n",
    "\n",
    "\n",
    "\n",
    "# Line profiling  # https://mortada.net/easily-profile-python-code-in-jupyter.html\n",
    "import line_profiler\n",
    "%load_ext line_profiler\n",
    "\n",
    "# Parsing\n",
    "import geopandas\n",
    "# import descartes\n",
    "\n",
    "# Map display\n",
    "import folium\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import branca.element as bre\n",
    "from branca.colormap import LinearColormap\n",
    "\n",
    "# FSA correspondence\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import time\n",
    "#import shapely\n",
    "import dill # for saving/loading results to save on computation time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data structure\n",
    "\n",
    "To facilitate automation of iteration through cities, we will organize each city's information within a dict and arrange those dicts in a list:\n",
    "\n",
    "'cities' is a list of dicts that each have entries:\n",
    "\n",
    "    name:        The name of the city\n",
    "    group:       The name of the city group; all groups have the same geographic data source and processing procedure\n",
    "    gdf:         A geopandas geodataframe, collects geometric unit information like area, population, etc.\n",
    "    geojson:     The json representation of the geometry in gdf, with only required properties\n",
    "    centroid:    [lat,long] of the centriod of all geometries in gdf\n",
    "    bounds:      List of two [lat,long] lists describing southwest and northeast corners of a gdf bounding box\n",
    "    df_ven:      A pandas dataframe, collects venue information, keyed to DAUID\n",
    "    df_features: A pandas dataframe with only final clustering features, keyed to DAUID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "citynames_CA = ['Toronto','Montréal','Vancouver','Halifax']\n",
    "citygroup_CA = 'Canada'\n",
    "for cityname in citynames_CA:\n",
    "    if not cityname in [city['name'] for city in cities]:\n",
    "        cities.append({'name':cityname, 'group':citygroup_CA})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Geographic Processing (Canada)\n",
    "\n",
    "[Statistics Canada](https://www.statcan.gc.ca/eng/start) provides [boundary files](https://www12.statcan.gc.ca/census-recensement/2011/geo/bound-limit/bound-limit-2016-eng.cfm) for several scales of geographic division for which census statistics are reported.  Each bounded area is labeled with descriptive [attributes](https://www150.statcan.gc.ca/n1/pub/92-160-g/2016002/tbl/tbl_4.13-eng.htm).  The smallest area for which all statistics are reported is the Dissemination Area, the files for which include the following heirarchical labels:\n",
    "\n",
    "Abbrev. | Name | Population | Description\n",
    "---|---|---|---\n",
    "DA | Dissemination Area | 400-700 | Composed of Dissemination Blocks.  Smallest standard geographic area for which all census data are disseminated\n",
    "ADA | Aggregate Dissemination Area | 5,000-15,000 | Cover the entire country.  Respect PR, CD, CMA/CA, CT boundaries.\n",
    "CT | Census Tract | <10,000 | Located in CMA/CA with core population >=50,000\n",
    "CMA | Census Metropolitan Area / Census Agglomeration | >50,000 / >10,000 | Formed by adjacent municipalities around core with high degree of integration\n",
    "CSD | Census Subdivision | | Municipality or equivalent\n",
    "CCS | Consolidated Census Subdivision | | Adjacent CSD within same CD\n",
    "CD | Census Division | | Neighboring municipalities linked for regional planning and service management\n",
    "ER | Economic Region | | Group of complete CD (one exception in Ontario) created for regional economic analysis\n",
    "PR | Province/Territory | | Province or Territory\n",
    "\n",
    "We will begin by looking at Distribution Areas as proxies for neighborhoods for cities in Canada.  In previous work where the Forward Sortation Areas (first three characters of the postal code) were used as neighborhood proxies, the sizes of many areas were quite large (several kilometers across) and therefore are likely internally non-homogeneous from a features perspective at the walking-distance (500 m) length scale.  To convert our chosen geographic areas to neighborhood names we could use the [FSA boundary file](https://www12.statcan.gc.ca/census-recensement/2011/geo/bound-limit/bound-limit-2016-eng.cfm) intersection with the DA to find the appropriate FSA, then look up the associated neighborhood names from [this](https://en.wikipedia.org/wiki/Demographics_of_Toronto_neighbourhoods) Wikipedia page.  A source for neighborhood boundary files remains an open question.\n",
    "\n",
    "File lda_000b16g_e.gml was downloaded from the [Statistics Canada: Boundary Files](https://www12.statcan.gc.ca/census-recensement/2011/geo/bound-limit/bound-limit-2016-eng.cfm) website.\n",
    "\n",
    "Exploring the gml file and computing the area and centroid of the distribution areas can be done using the [geopandas module](https://geopandas.org/).  Geopandas builds upon [osgeo](https://gdal.org/python/index.html) which can also be used to explore and compute with the gml file, but in testing geopandas was 46 times faster due to vectorization of many calculations compared to a naive approach (see Appendix).\n",
    "\n",
    "Latitude and Longitude need to be obtained from a projection with those units, e.g. [EPSG-4326](https://epsg.io/4326).  Area can be calculated from an equal-area projection, e.g. [EPSG-6931](https://epsg.io/6931) (though a geodesic area calculation would be more accurate for larger regions, that would have to come from an additional package such as [proj](https://proj.org/), but since all regions here are small such the curvature of the earth is negligible, and altitude indtroduces an additional error likely comparable or larger to the curvature error, we will proceed with a simpler equal-area projection calculation).  The geometry is saved as text in the [Well-Known Text (WKT)](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry) representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import using geopandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def GMLtoGDF(filename):\n",
    "    gdf = geopandas.read_file(filename)\n",
    "    gdf.rename_geometry('Geometry', inplace=True) # Default geometry column name is 'geometry'; changed for consistent capitalization of columns\n",
    "    gdf.set_geometry('Geometry') # Renaming is insufficient; this sets special variable gdf.geometry = gdf['Geometry']\n",
    "    gdf = gdf.set_crs(epsg=3347) # Needed only for FSA file, the others are 3347 and parsed correctly by geopandas, and the pdf in the zip file has the same projection parameters (FSA vs. DA, ADA, CT)\n",
    "    gdf['Area'] = gdf['Geometry'].to_crs(epsg=6931).area # Equal-area projection\n",
    "    gdf['Centroid'] = gdf['Geometry'].centroid\n",
    "    gdf['Geometry'] = gdf['Geometry'].to_crs(epsg=4326) # Latitude/Longitude representation\n",
    "    gdf['Centroid'] = gdf['Centroid'].to_crs(epsg=4326) # Only the set geometry is converted with gdf.to_crs(); all other geometry-containing columns must be converted explicitly; here we convert all columns explicitly\n",
    "    gdf = gdf.set_crs(epsg=4326) # The series and geodataframe can have separate crs; this was found necessary for the geopandas.union function to operate easily\n",
    "    gdf['Centroid Latitude'] = gdf['Centroid'].geometry.y\n",
    "    gdf['Centroid Longitude'] = gdf['Centroid'].geometry.x\n",
    "    gdf.drop(columns = 'Centroid', inplace=True) # Because WKT Point cannot be serialized to JSON, we drop the Centroid column and keep only its float components\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gdf_CA_DA = GMLtoGDF(DIR_DATA+'lda_000b16g_e.gml') # CA for Canada, DA for Dissemination Area level\n",
    "gdf_CA_DA.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract geojson for each city\n",
    "\n",
    "We will use folium, which can display patches provided in a geojson format, which is trivial to get using geopandas.  Note that points cannot be serialized (what!?) so the centroid must be dropped.  Furthermore, when coloring the patches, folium uses a dataframe keyed to the patches (for the Choropleth class this is optional; for the GeoJson class the values can be extracted from the json directly).  Therefore for convenience we split the geodataframe into patch and data components.\n",
    "\n",
    "We define some utility functions for conditionally extracting the geojson and selecting cities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fillCity(city, gdf, fieldname, method='equals', names=None):\n",
    "    '''Populates the city dict using data from gdf, selection method, and names\n",
    "    \n",
    "    Expects city keys: name, group\n",
    "    Populates city keys: gdf, geojson, data, centroid, bounds, selection\n",
    "        geojson contains keys DAUID, Geometry, Area\n",
    "        centroid and bounds use [latitude,longitude] format\n",
    "    \n",
    "    fieldname must be a column name in gdf used for selection\n",
    "    method determines how the fieldname entry is used for row selection, with respect to the city name\n",
    "        'equals': names is None (uses city name) or a list of strings at least one of which must match fieldname exactly\n",
    "        'contains': names is None (uses city name) or a list of strings at least one of which must appear in fieldname\n",
    "    \n",
    "    Will overwrite existing entries\n",
    "    '''\n",
    "    if names==None:\n",
    "        names = [city['name']]\n",
    "    gdf_new = selectRegion(gdf, fieldname, method, names)\n",
    "    updateCity(city, gdf_new)\n",
    "    city['selection'] = f'{fieldname} {method} {names}'\n",
    "\n",
    "def selectRegion(gdf, fieldname, method='equals', names=None):\n",
    "    '''Selects rows from gdf using method to compare entries in fieldname to names\n",
    "    \n",
    "    fieldname must be a column name in gdf used for selection\n",
    "    method determines how the fieldname entry is used for row selection, with respect to the city name\n",
    "        'equals': names is None (uses city name) or a list of strings at least one of which must match fieldname exactly\n",
    "        'contains': names is None (uses city name) or a list of strings at least one of which must appear in fieldname\n",
    "    '''\n",
    "    if names==None:\n",
    "        print('error: names must be a string or list of strings')\n",
    "        return None\n",
    "    if not type(names)==list:\n",
    "        if not type(names)==str:\n",
    "            print('error: names must be a string or list of strings')\n",
    "            return None\n",
    "        names = [names]\n",
    "    \n",
    "    select = False\n",
    "    for name in names:\n",
    "        if method=='equals':\n",
    "            select = select | (gdf[fieldname]==name)\n",
    "        elif method=='contains':\n",
    "            select = select | gdf[fieldname].str.contains(name)\n",
    "        else:\n",
    "            print(\"error: method must be 'equals' or 'contains'.\")\n",
    "            return\n",
    "    return gdf.loc[select,:]\n",
    "    \n",
    "def updateCity(city, gdf):\n",
    "    '''Populates the city dict using data from gdf\n",
    "    \n",
    "    Expects city keys: name, group\n",
    "    Populates city keys: gdf, geojson, data, centroid, bounds\n",
    "        geojson contains gdf keys except Centroid\n",
    "        centroid and bounds use [latitude,longitude] format\n",
    "    \n",
    "    Will overwrite existing entries\n",
    "    '''\n",
    "    city['gdf'] = gdf\n",
    "    city['geojson'] = gdf.copy(deep=True).to_json()\n",
    "    #city['data'] = gdf.drop(columns=['Geometry']).copy(deep=True)\n",
    "    tmp = gdf.geometry.unary_union\n",
    "    city['centroid'] = [tmp.centroid.y,tmp.centroid.x]\n",
    "    tmp = tmp.envelope.boundary.coords.xy\n",
    "    city['bounds'] = [[min(tmp[1]),min(tmp[0])],[max(tmp[1]),max(tmp[0])]]\n",
    "\n",
    "def getCityByName(cityname):\n",
    "    '''Returns the first city dict that matches cityname exactly'''\n",
    "    for city in cities:\n",
    "        if city['name']==cityname:\n",
    "            return city\n",
    "    return None\n",
    "\n",
    "def getCityGroup(groupname):\n",
    "    '''Returns the subset of cities where group matches groupname exactly'''\n",
    "    ret = []\n",
    "    for city in cities:\n",
    "        if city['group']==groupname:\n",
    "            ret.append(city)\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can simply populate the city dicts by selecting only Dissemination Areas with Consolidated Census Subdivision Name equal to the city name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for city in cities:\n",
    "    if city['group']=='Canada':\n",
    "        fillCity(city,gdf_CA_DA,'CCSNAME',method='equals')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a quick inspection of the first entries of the resulting dataframes as a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', None)\n",
    "for city in cities:\n",
    "    print(city['name'],':',f\"{city['gdf'].shape[0]}\",'rows')\n",
    "    display(city['gdf'].head(1))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at how many census tracts have a given number of dissemination areas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histDAinCT(cities):\n",
    "    if type(cities)==dict:\n",
    "        cities = [cities]\n",
    "    n = len(cities)\n",
    "    plt.figure(figsize=(4*n,3))\n",
    "    for i, city in enumerate(cities):\n",
    "        plt.subplot(1,n,i+1)\n",
    "        plt.hist(city['gdf'].groupby(['CTNAME']).count()['DAUID'],bins=np.arange(-0.4,20.4,1),width=0.8)\n",
    "        plt.xlabel('# Dissemination Areas in a Census Tract')\n",
    "        plt.ylabel('Number of Census Tracts')\n",
    "        plt.title(f\"{city['name']}, {city['gdf'].shape[0]} Dissemination Areas\")\n",
    "        plt.xticks(np.arange(0,21,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histDAinCT(getCityGroup('Canada'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at the area distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DA Count per city:')\n",
    "count_DA = [(city['name'], city['gdf'].shape[0]) for city in cities]\n",
    "print(count_DA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize Dissemination Areas for each city\n",
    "\n",
    "This will be done using Folium.\n",
    "\n",
    "Note that there are some issues with intuitive rendering of multiple inline maps using Folium/Branca:\n",
    "* The colorbar is hard-coded to 500 px width (see ColorMap._repr_html_ in the [documentation](https://github.com/python-visualization/branca/blob/master/branca/colormap.py))\n",
    "* The colorbar is added to the first choropleth in the figure, not the choropleth that creates it.\n",
    "* LinearColormap.to_step requires input for variable 'data' in order to set logarithmic scale, and still the colormap is not displayed logarithmically\n",
    "\n",
    "To get around thes issues, we manually create a colormap to use for all choropleths, add a new div/map above the target row and add the colormap to it (because a colormap must be renedered from within a map), and add a second div for the title.  Actually fixing these issues would require editing the modules, a potential project for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCityBounds(cities, featurename):\n",
    "    '''Returns the global bounds of column featurename across all cities\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cities: dict or list of dicts as defined above\n",
    "    featurename: str column name in cities[i]['gdf'] for which bounds will be obtained\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (min, max) value across all cities\n",
    "    '''\n",
    "    if type(cities)==dict:\n",
    "        cities = [cities]\n",
    "    bounds = [[],[]]\n",
    "    for city in cities:\n",
    "        bounds[0].append(city['gdf'][featurename].min())\n",
    "        bounds[1].append(city['gdf'][featurename].max())\n",
    "    bounds[0] = min(bounds[0])\n",
    "    bounds[1] = max(bounds[1])\n",
    "    return bounds\n",
    "\n",
    "def extendBound(bound,direction='up',method='nearestLeadingDigit',scale=10):\n",
    "    '''Extend bound to next 'round' number\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bound: float or float castable number or a list thereof\n",
    "    direction: {'up','down',nonzero number} or a list of these values indicating the direction to round in\n",
    "    method: str describing the extension method\n",
    "        'nearestLeadingDigit': Bound is nearest numbers with leading digit followed by zeros\n",
    "        'nearestPower': Bound is nearest integer power of scale (scale must be > 1).  For negative numbers, the sign and direction are reversed, the extension performed, then the sign of the result is reversed back.\n",
    "        'nearestMultiple': Bound is nearest multiple of scale (scale must be > 0)\n",
    "        'round': Bound is rounded using the default method\n",
    "    scale: numeric as described in method options or a list thereof\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float: the extended bound\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    All inputs, if not single-valued, must be list-castable and of equal length\n",
    "    If all inputs are single-valued, the output is a float, otherwise it is a list of floats\n",
    "    '''\n",
    "    import numpy as np\n",
    "    \n",
    "    # Check and adjust the length of inputs\n",
    "    unlist = False\n",
    "    try:\n",
    "        bound = list(bound)\n",
    "    except:\n",
    "        try:\n",
    "            bound = [bound]\n",
    "            unlist = True\n",
    "        except:\n",
    "            print(\"Input 'bound' must be numeric or convertible to list type.\")\n",
    "            return None\n",
    "    try:\n",
    "        if type(direction)==str:\n",
    "            direction = [direction]\n",
    "        direction = list(direction)\n",
    "    except:\n",
    "        try:\n",
    "            direction = [direction]\n",
    "        except:\n",
    "            print(\"Input 'direction' must be a string or nonzero number or convertible to list type.\")\n",
    "            return None\n",
    "    try:\n",
    "        if type(method)==str:\n",
    "            method = [method]\n",
    "        method = list(method)\n",
    "    except:\n",
    "        try:\n",
    "            method = [method]\n",
    "        except:\n",
    "            print(\"Input 'method' must be a string or convertible to list type.\")\n",
    "            return None\n",
    "    try:\n",
    "        scale = list(scale)\n",
    "    except:\n",
    "        try:\n",
    "            scale = [scale]\n",
    "        except:\n",
    "            print(\"Input 'scale' must be numeric or convertible to list type.\")\n",
    "            return None\n",
    "    inputs = [bound, direction, method, scale]\n",
    "    lengths = [len(i) for i in inputs]\n",
    "    set_lengths = set(lengths)\n",
    "    max_len = max(set_lengths)\n",
    "    set_lengths.remove(1)\n",
    "    if len(set_lengths)>1:\n",
    "        print('Inputs must be of the same length or of length one.  See help(extendBound)')\n",
    "        return None\n",
    "    if max_len>1: # can this be converted to a looped statement?\n",
    "        if len(bound)==1:\n",
    "            bound = bound*max_len\n",
    "        if len(direction)==1:\n",
    "            direction = direction*max_len\n",
    "        if len(method)==1:\n",
    "            method = method*max_len\n",
    "        if len(scale)==1:\n",
    "            scale = scale*max_len\n",
    "        unlist = False\n",
    "\n",
    "    # If multiple methods are specified, recursively call this function for each method and reassemble results\n",
    "    if len(bound)>1 and len(set(method))>1:\n",
    "        ret = np.array([None for b in bound])\n",
    "        for m in list(set(method)):\n",
    "            ind = np.where(np.array(method)==m)\n",
    "            ret[ind] = extendBound(list(np.array(bound)[ind]),list(np.array(direction)[ind]),m,list(np.array(scale)[ind]))\n",
    "        return list(ret)\n",
    "    \n",
    "    # Convert direction to a logical array roundup\n",
    "    try:\n",
    "        roundup = [True if d=='up' else False if d=='down' else True if float(d)>0 else False if float(d)<0 else None for d in direction]\n",
    "    except:\n",
    "        print('direction must be \"up\", \"down\", or a non-negative number')\n",
    "        return None\n",
    "    if any([r==None for r in roundup]):\n",
    "        print('direction must be \"up\", \"down\", or a non-negative number')\n",
    "        return None\n",
    "    \n",
    "    # Cases for multiple methods handled above, return to string method\n",
    "    method = method[0]\n",
    "    \n",
    "    # Execute the conversions\n",
    "    if method=='nearestLeadingDigit':\n",
    "        iszero = np.array(bound)==0\n",
    "        isnegative = np.array(bound) < 0\n",
    "        offsets = np.logical_xor(roundup, isnegative)\n",
    "        power = [0 if z else np.floor(np.log10(abs(b))) for b, z in zip(bound, iszero)]\n",
    "        firstdigit = [abs(b)//np.power(10,p) for b, p in zip(bound, power)]\n",
    "        exceeds = [abs(b)>f*np.power(10,p) for b, f, p in zip(bound, firstdigit, power)]\n",
    "        newbound = [abs(b) if not t else (f+o)*np.power(10,p) for b, t, n, f, o, p in zip(bound, exceeds, isnegative, firstdigit, offsets, power)]\n",
    "        newbound = [-n if t else n for n, t in zip(newbound,isnegative)]\n",
    "    elif method=='nearestPower':\n",
    "        try:\n",
    "            scale = [float(s) for s in scale]\n",
    "            if any([s<=1 for s in scale]):\n",
    "                print('scale should be greater than 1')\n",
    "                return None\n",
    "        except ValueError:\n",
    "            print('scale should be a number or list of numbers greater than 1')\n",
    "            return None\n",
    "        isnegative = np.array(bound) < 0\n",
    "        offsets = np.logical_xor(roundup, isnegative)\n",
    "        roundfuns = [np.ceil if o else np.floor for o in offsets]\n",
    "        newbound = [0 if b==0 else np.power(s, r(np.log10(abs(b))/np.log10(s))) for b, r, s in zip(bound,roundfuns,scale)]\n",
    "        newbound = [-n if t else n for n, t in zip(newbound,isnegative)]\n",
    "    elif method=='nearestMultiple':\n",
    "        try:\n",
    "            scale = [float(s) for s in scale]\n",
    "            if any([s<=0 for s in scale]):\n",
    "                print('scale should be greater than 0')\n",
    "                return None\n",
    "        except ValueError:\n",
    "            print('scale should be a number or list of numbers greater than 0')\n",
    "            return None\n",
    "        roundfuns = [np.ceil if r else np.floor for r in roundup]\n",
    "        newbound = [s*(r(b/s)) for b, r, s in zip(bound,roundfuns,scale)]\n",
    "    elif method=='round':\n",
    "        roundfuns = [np.ceil if r else np.floor for r in roundup]\n",
    "        newbound = [f(b) for b, f in zip(bound, roundfuns)]\n",
    "    else:\n",
    "        print('Invalid method, see help(extendBound)')\n",
    "        return None\n",
    "    return newbound[0] if unlist else newbound\n",
    "\n",
    "def extendBounds(bounds,method='nearestLeadingDigit',scale=10):\n",
    "    if bounds[0]>bounds[1]:\n",
    "        print('bounds must be ordered from least to greatest')\n",
    "        return None    \n",
    "    return extendBound(bounds,direction=['down','up'],method=method,scale=scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing function extendBounds\n",
      "\n",
      "Testing invalid method\n",
      "  Expect errors:\n",
      "Invalid method, see help(extendBound)\n",
      "Input: [11, 130] invalid 10   Output: None   Expected: None   Passed: True\n",
      "\n",
      "Testing method 'nearestLeadingDigit'\n",
      "  Expect errors:\n",
      "bounds must be ordered from least to greatest\n",
      "Input: [9, -930] nearestLeadingDigit 10   Output: None   Expected: None   Passed: True\n",
      "bounds must be ordered from least to greatest\n",
      "Input: [-9, -930] nearestLeadingDigit 10   Output: None   Expected: None   Passed: True\n",
      "  Expect success:\n",
      "Input: [11, 130] nearestLeadingDigit 10   Output: [10.0, 200.0]   Expected: [10, 200]   Passed: True\n",
      "Input: [11, 130] nearestLeadingDigit -1   Output: [10.0, 200.0]   Expected: [10, 200]   Passed: True\n",
      "Input: [9, 930] nearestLeadingDigit 10   Output: [9, 1000.0]   Expected: [9, 1000]   Passed: True\n",
      "Input: [-9, 930] nearestLeadingDigit 10   Output: [-9, 1000.0]   Expected: [-9, 1000]   Passed: True\n",
      "Input: [-990, -930] nearestLeadingDigit 10   Output: [-1000.0, -900.0]   Expected: [-1000, -900]   Passed: True\n",
      "Input: [-990, 0.05] nearestLeadingDigit 10   Output: [-1000.0, 0.05]   Expected: [-1000, 0.05]   Passed: True\n",
      "Input: [0, 0.052] nearestLeadingDigit 10   Output: [0, 0.06]   Expected: [0, 0.06]   Passed: True\n",
      "\n",
      "Testing method 'nearestPower'\n",
      "  Expect errors:\n",
      "scale should be greater than 1\n",
      "Input: [11, 130] nearestPower -2   Output: None   Expected: None   Passed: True\n",
      "scale should be greater than 1\n",
      "Input: [11, 130] nearestPower 0   Output: None   Expected: None   Passed: True\n",
      "scale should be greater than 1\n",
      "Input: [11, 130] nearestPower 1   Output: None   Expected: None   Passed: True\n",
      "bounds must be ordered from least to greatest\n",
      "Input: [-11, -130] nearestPower 10   Output: None   Expected: None   Passed: True\n",
      "  Expect success:\n",
      "Input: [11, 130] nearestPower 10   Output: [10.0, 1000.0]   Expected: [10, 1000]   Passed: True\n",
      "Input: [10, 100] nearestPower 10   Output: [10.0, 100.0]   Expected: [10, 100]   Passed: True\n",
      "Input: [11, 130] nearestPower 1.1   Output: [10.834705943388395, 142.04293198443193]   Expected: [10.834705943388395, 142.04293198443193]   Passed: True\n",
      "Input: [11, 130] nearestPower 2   Output: [8.0, 256.0]   Expected: [8, 256]   Passed: True\n",
      "Input: [11, 130] nearestPower 10   Output: [10.0, 1000.0]   Expected: [10, 1000]   Passed: True\n",
      "Input: [11, 130] nearestPower 10.0   Output: [10.0, 1000.0]   Expected: [10, 1000]   Passed: True\n",
      "Input: [-11, 130] nearestPower 10   Output: [-100.0, 1000.0]   Expected: [-100, 1000]   Passed: True\n",
      "Input: [-5100, -130] nearestPower 10   Output: [-10000.0, -100.0]   Expected: [-10000, -100]   Passed: True\n",
      "Input: [-0.0101, -0.00042] nearestPower 10   Output: [-0.1, -0.0001]   Expected: [-0.1, -0.0001]   Passed: True\n",
      "Input: [0, 0.00042] nearestPower 10   Output: [0, 0.001]   Expected: [0, 0.001]   Passed: True\n",
      "\n",
      "Testing method 'nearestMultiple'\n",
      "  Expect errors:\n",
      "scale should be greater than 0\n",
      "Input: [11, 132] nearestMultiple -2   Output: None   Expected: None   Passed: True\n",
      "scale should be greater than 0\n",
      "Input: [11, 132] nearestMultiple 0   Output: None   Expected: None   Passed: True\n",
      "bounds must be ordered from least to greatest\n",
      "Input: [0, -10] nearestMultiple 100   Output: None   Expected: None   Passed: True\n",
      "  Expect success:\n",
      "Input: [11, 132] nearestMultiple 10   Output: [10.0, 140.0]   Expected: [10, 140]   Passed: True\n",
      "Input: [10, 130] nearestMultiple 10   Output: [10.0, 130.0]   Expected: [10, 130]   Passed: True\n",
      "Input: [11.55, 132.55] nearestMultiple 0.1   Output: [11.5, 132.6]   Expected: [11.5, 132.6]   Passed: True\n",
      "Input: [11.55, 132.55] nearestMultiple 1   Output: [11.0, 133.0]   Expected: [11, 133]   Passed: True\n",
      "Input: [11.55, 132.55] nearestMultiple 100   Output: [0.0, 200.0]   Expected: [0, 200]   Passed: True\n",
      "Input: [-11, 132] nearestMultiple 10   Output: [-20.0, 140.0]   Expected: [-20, 140]   Passed: True\n",
      "Input: [-1121, -132] nearestMultiple 10   Output: [-1130.0, -130.0]   Expected: [-1130, -130]   Passed: True\n",
      "Input: [-10, -10] nearestMultiple 10   Output: [-10.0, -10.0]   Expected: [-10, -10]   Passed: True\n",
      "Input: [-10, -10] nearestMultiple 100   Output: [-100.0, -0.0]   Expected: [-100, 0]   Passed: True\n",
      "\n",
      "Testing method 'round'\n",
      "  Expect errors:\n",
      "bounds must be ordered from least to greatest\n",
      "Input: [-11.1, -132.1] round 10   Output: None   Expected: None   Passed: True\n",
      "  Expect success:\n",
      "Input: [11.1, 132.1] round 10   Output: [11.0, 133.0]   Expected: [11, 133]   Passed: True\n",
      "Input: [10, 130] round 10   Output: [10.0, 130.0]   Expected: [10, 130]   Passed: True\n",
      "Input: [11.1, 132.1] round -2   Output: [11.0, 133.0]   Expected: [11, 133]   Passed: True\n",
      "Input: [-11.1, 132.1] round 10   Output: [-12.0, 133.0]   Expected: [-12, 133]   Passed: True\n",
      "Input: [-1100.1, -132.1] round 10   Output: [-1101.0, -132.0]   Expected: [-1101, -132]   Passed: True\n",
      "\n",
      "Testing array execution\n",
      "  Expect errors:\n",
      "  Expect success:\n",
      "    method\n",
      "Input: 1.5 up nearestLeadingDigit 4   Output: 2.0   Expected: 2   Passed: True\n",
      "Input: 1.5 up nearestPower 4   Output: 4.0   Expected: 4   Passed: True\n",
      "Input: 1.5 up nearestMultiple 4   Output: 4.0   Expected: 4   Passed: True\n",
      "Input: 1.5 up round 4   Output: 2.0   Expected: 2   Passed: True\n",
      "    direction\n",
      "Input: 1.5 up nearestMultiple 4   Output: 4.0   Expected: 4   Passed: True\n",
      "Input: 1.5 0.1 nearestMultiple 4   Output: 4.0   Expected: 4   Passed: True\n",
      "Input: 1.5 down nearestMultiple 4   Output: 0.0   Expected: 0   Passed: True\n",
      "Input: 1.5 -0.1 nearestMultiple 4   Output: 0.0   Expected: 0   Passed: True\n",
      "    broadcasting\n",
      "Input: [1.5] up nearestMultiple 1   Output: [2.0]   Expected: [2]   Passed: True\n",
      "Input: [1.5, 2.5] up nearestMultiple 1   Output: [2.0, 3.0]   Expected: [2, 3]   Passed: True\n",
      "Input: 1.5 ['up', 'down'] nearestMultiple 1   Output: [2.0, 1.0]   Expected: [2, 1]   Passed: True\n",
      "Input: 1.5 up ['nearestLeadingDigit', 'nearestPower', 'nearestMultiple', 'round'] 3   Output: [2.0, 3.0, 3.0, 2.0]   Expected: [2, 3, 3, 2]   Passed: True\n",
      "Input: 1.5 up nearestMultiple [1, 5, 10]   Output: [2.0, 5.0, 10.0]   Expected: [2, 5, 10]   Passed: True\n",
      "\n",
      "All tests passed: True\n"
     ]
    }
   ],
   "source": [
    "# Unit test of extendBounds\n",
    "#   TODO: check out the builtin unittest module and convert this code to use that testing structure\n",
    "\n",
    "# Get default arguments from https://stackoverflow.com/questions/12627118/get-a-function-arguments-default-value\n",
    "import inspect\n",
    "def get_default_args(func):\n",
    "    signature = inspect.signature(func)\n",
    "    return {\n",
    "        k: v.default\n",
    "        for k, v in signature.parameters.items()\n",
    "        if v.default is not inspect.Parameter.empty\n",
    "    }\n",
    "\n",
    "defaults = get_default_args(extendBound)\n",
    "def test_extendBound(bounds,direction=defaults['direction'],method=defaults['method'],scale=defaults['scale'],expected=None): # default arguments taken from extendBound; not sure how to get defaults when not supplied\n",
    "    output = extendBound(bounds,direction,method,scale)\n",
    "    print('Input:',bounds,direction,method,scale,'  Output:',output,'  Expected:',expected,'  Passed:',output==expected)\n",
    "    return output==expected\n",
    "\n",
    "defaults = get_default_args(extendBounds)\n",
    "def test_extendBounds(bounds,method=defaults['method'],scale=defaults['scale'],expected=None): # default arguments taken from extendBounds; not sure how to get defaults when not supplied\n",
    "    output = extendBounds(bounds,method,scale)\n",
    "    print('Input:',bounds,method,scale,'  Output:',output,'  Expected:',expected,'  Passed:',output==expected)\n",
    "    return output==expected\n",
    "\n",
    "print('Testing function extendBounds\\n')\n",
    "\n",
    "passed = True\n",
    "print(\"Testing invalid method\")\n",
    "print(\"  Expect errors:\")\n",
    "passed = test_extendBounds([11,130],'invalid',expected=None) and passed\n",
    "print()\n",
    "\n",
    "print(\"Testing method 'nearestLeadingDigit'\")\n",
    "print(\"  Expect errors:\")\n",
    "passed = test_extendBounds([9,-930],'nearestLeadingDigit',expected=None) and passed\n",
    "passed = test_extendBounds([-9,-930],'nearestLeadingDigit',expected=None) and passed\n",
    "print(\"  Expect success:\")\n",
    "passed = test_extendBounds([11,130],'nearestLeadingDigit',expected=[10,200]) and passed\n",
    "passed = test_extendBounds([11,130],'nearestLeadingDigit',-1,expected=[10,200]) and passed\n",
    "passed = test_extendBounds([9,930],'nearestLeadingDigit',expected=[9,1000]) and passed\n",
    "passed = test_extendBounds([-9,930],'nearestLeadingDigit',expected=[-9,1000]) and passed\n",
    "passed = test_extendBounds([-990,-930],'nearestLeadingDigit',expected=[-1000,-900]) and passed\n",
    "passed = test_extendBounds([-990,0.05],'nearestLeadingDigit',expected=[-1000,0.05]) and passed\n",
    "passed = test_extendBounds([0,0.052],'nearestLeadingDigit',expected=[0,0.06]) and passed\n",
    "print()\n",
    "\n",
    "print(\"Testing method 'nearestPower'\")\n",
    "print(\"  Expect errors:\")\n",
    "passed = test_extendBounds([11,130],'nearestPower',-2,expected=None) and passed\n",
    "passed = test_extendBounds([11,130],'nearestPower',0,expected=None) and passed\n",
    "passed = test_extendBounds([11,130],'nearestPower',1,expected=None) and passed\n",
    "passed = test_extendBounds([-11,-130],'nearestPower',10,expected=None) and passed\n",
    "print(\"  Expect success:\")\n",
    "passed = test_extendBounds([11,130],'nearestPower',expected=[10,1000]) and passed\n",
    "passed = test_extendBounds([10,100],'nearestPower',expected=[10,100]) and passed\n",
    "passed = test_extendBounds([11,130],'nearestPower',1.1,expected=[10.834705943388395, 142.04293198443193]) and passed\n",
    "passed = test_extendBounds([11,130],'nearestPower',2,expected=[8,256]) and passed\n",
    "passed = test_extendBounds([11,130],'nearestPower',10,expected=[10,1000]) and passed\n",
    "passed = test_extendBounds([11,130],'nearestPower',10.,expected=[10,1000]) and passed\n",
    "passed = test_extendBounds([-11,130],'nearestPower',10,expected=[-100,1000]) and passed\n",
    "passed = test_extendBounds([-5100,-130],'nearestPower',10,expected=[-10000,-100]) and passed\n",
    "passed = test_extendBounds([-.0101,-0.00042],'nearestPower',10,expected=[-0.1,-0.0001]) and passed\n",
    "passed = test_extendBounds([0,0.00042],'nearestPower',10,expected=[0,0.001]) and passed\n",
    "print()\n",
    "\n",
    "print(\"Testing method 'nearestMultiple'\")\n",
    "print(\"  Expect errors:\")\n",
    "passed = test_extendBounds([11,132],'nearestMultiple',-2,expected=None) and passed\n",
    "passed = test_extendBounds([11,132],'nearestMultiple',0,expected=None) and passed\n",
    "passed = test_extendBounds([0,-10],'nearestMultiple',100,expected=None) and passed\n",
    "print(\"  Expect success:\")\n",
    "passed = test_extendBounds([11,132],'nearestMultiple',expected=[10,140]) and passed\n",
    "passed = test_extendBounds([10,130],'nearestMultiple',expected=[10,130]) and passed\n",
    "passed = test_extendBounds([11.55,132.55],'nearestMultiple',0.1,expected=[11.5,132.6]) and passed\n",
    "passed = test_extendBounds([11.55,132.55],'nearestMultiple',1,expected=[11,133]) and passed\n",
    "passed = test_extendBounds([11.55,132.55],'nearestMultiple',100,expected=[0,200]) and passed\n",
    "passed = test_extendBounds([-11,132],'nearestMultiple',10,expected=[-20,140]) and passed\n",
    "passed = test_extendBounds([-1121,-132],'nearestMultiple',10,expected=[-1130,-130]) and passed\n",
    "passed = test_extendBounds([-10,-10],'nearestMultiple',10,expected=[-10,-10]) and passed\n",
    "passed = test_extendBounds([-10,-10],'nearestMultiple',100,expected=[-100,0]) and passed\n",
    "print()\n",
    "\n",
    "print(\"Testing method 'round'\")\n",
    "print(\"  Expect errors:\")\n",
    "passed = test_extendBounds([-11.1,-132.1],'round',expected=None) and passed\n",
    "print(\"  Expect success:\")\n",
    "passed = test_extendBounds([11.1,132.1],'round',expected=[11,133]) and passed\n",
    "passed = test_extendBounds([10,130],'round',expected=[10,130]) and passed\n",
    "passed = test_extendBounds([11.1,132.1],'round',-2,expected=[11,133]) and passed\n",
    "passed = test_extendBounds([-11.1,132.1],'round',expected=[-12,133]) and passed\n",
    "passed = test_extendBounds([-1100.1,-132.1],'round',expected=[-1101,-132]) and passed\n",
    "print()\n",
    "\n",
    "print(\"Testing array execution\")\n",
    "print(\"  Expect errors:\")\n",
    "print(\"  Expect success:\")\n",
    "print(\"    method\")\n",
    "passed = test_extendBound(1.5,'up','nearestLeadingDigit',4,expected=2) and passed\n",
    "passed = test_extendBound(1.5,'up','nearestPower',4,expected=4) and passed\n",
    "passed = test_extendBound(1.5,'up','nearestMultiple',4,expected=4) and passed\n",
    "passed = test_extendBound(1.5,'up','round',4,expected=2) and passed\n",
    "print(\"    direction\")\n",
    "passed = test_extendBound(1.5,'up','nearestMultiple',4,expected=4) and passed\n",
    "passed = test_extendBound(1.5,0.1,'nearestMultiple',4,expected=4) and passed\n",
    "passed = test_extendBound(1.5,'down','nearestMultiple',4,expected=0) and passed\n",
    "passed = test_extendBound(1.5,-0.1,'nearestMultiple',4,expected=0) and passed\n",
    "print(\"    broadcasting\")\n",
    "passed = test_extendBound([1.5],'up','nearestMultiple',1,expected=[2]) and passed\n",
    "passed = test_extendBound([1.5,2.5],'up','nearestMultiple',1,expected=[2,3]) and passed\n",
    "passed = test_extendBound(1.5,['up','down'],'nearestMultiple',1,expected=[2,1]) and passed\n",
    "passed = test_extendBound(1.5,'up',['nearestLeadingDigit','nearestPower','nearestMultiple','round'],3,expected=[2,3,3,2]) and passed\n",
    "passed = test_extendBound(1.5,'up','nearestMultiple',[1,5,10],expected=[2,5,10]) and passed\n",
    "print()\n",
    "\n",
    "print(\"All tests passed:\",passed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mapCitiesAdjacent(cities,propertyname,title='',tooltiplabels=None):\n",
    "    '''Displays cities in separate adjacent maps\n",
    "    \n",
    "    Cities dict as above\n",
    "    Displayed as choropleth keyed to propertyname\n",
    "    Header displays title and colormap labeled with keyed propertyname\n",
    "    Tooltips pop up on mouseover showing properties listed in tooltiplabels\n",
    "    \n",
    "    Inspired by https://gitter.im/python-visualization/folium?at=5a36090a03838b2f2a04649d\n",
    "    \n",
    "    Assumes propertyname is identical in gdf and geojson\n",
    "    '''\n",
    "    if (not type(cities)==list) and type(cities)==dict:\n",
    "        cities = [cities]\n",
    "    \n",
    "    f = bre.Figure()\n",
    "    div_header = bre.Div(position='absolute',height='10%',width='100%',left='0%',top='0%').add_to(f)\n",
    "\n",
    "    map_header = folium.Map(location=[0,0],control_scale=False,zoom_control=False,tiles=None,attr=False).add_to(div_header)\n",
    "    div_header2 = bre.Div(position='absolute',height='10%',width='97%',left='3%',top='0%').add_to(div_header)\n",
    "    html_header = '''<h3 align=\"left\" style=\"font-size:16px;charset=utf-8\"><b>{}</b></h3>'''.format(title)\n",
    "    div_header2.get_root().html.add_child(folium.Element(html_header))\n",
    "\n",
    "    vbounds = getCityBounds(cities,propertyname)\n",
    "    vbounds[0] = 0\n",
    "    vbounds = extendBounds(vbounds,'nearestLeadingDigit')\n",
    "    \n",
    "    cm_header = LinearColormap(\n",
    "        colors=['yellow', 'orange', 'red'],\n",
    "        index=None,\n",
    "        vmin=vbounds[0],\n",
    "        vmax=vbounds[1],\n",
    "        caption=propertyname\n",
    "        ).add_to(map_header) # .to_step(method='log', n=5, data=?), log has log labels but linear color scale\n",
    "\n",
    "    for i, city in enumerate(cities):\n",
    "        div_map = bre.Div(position='absolute',height='80%',width=f'{(100/len(cities))}%',left=f'{(i*100/len(cities))}%',top='10%').add_to(f)\n",
    "\n",
    "        city_map = folium.Map(location=city['centroid'], control_scale=True)\n",
    "        div_map.add_child(city_map)\n",
    "        title_html = '''<h3 align=\"center\" style=\"font-size:16px;charset=utf-8\"><b>{}</b></h3>'''.format(city['name'])\n",
    "        city_map.get_root().html.add_child(folium.Element(title_html))\n",
    "\n",
    "        city_map.fit_bounds(city['bounds'])\n",
    "\n",
    "        m = folium.GeoJson(\n",
    "                    city['geojson'],\n",
    "                    style_function=lambda feature: {\n",
    "                        'fillColor': cm_header.rgb_hex_str(feature['properties'][propertyname]),\n",
    "                        'fillOpacity': 0.8,\n",
    "                        'color':'black',\n",
    "                        'weight': 1,\n",
    "                        'opacity': 0.2,\n",
    "                    },\n",
    "                    name=f'Choropleth_{i}'\n",
    "                ).add_to(city_map)\n",
    "        if not tooltiplabels==None:\n",
    "            m.add_child(folium.features.GeoJsonTooltip(tooltiplabels))\n",
    "    \n",
    "    display(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tooltip_DA = ['PRNAME','ERNAME','CDNAME','CCSNAME','CSDNAME','CMANAME','CDNAME','CTNAME','ADAUID','DAUID']\n",
    "mapCitiesAdjacent(getCityGroup('Canada'),'Area','Canadian Cities, Dissemination Areas colored by square meters',tooltip_DA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Toronto and Montréal look like reasonable areas.  Vancouver and Halifax could use some modification to make their areas of extent and density similar to Toronto and Montréal.\n",
    "\n",
    "First let's examine Vancouver.  We extend the region to include adjacent Census Subdivisions that that remain near the city center."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityname = 'Vancouver'\n",
    "gdf_select = gdf_CA_DA.loc[gdf_CA_DA['PRNAME'].str.contains('British Columbia'),:]\n",
    "# print('ERNAME', gdf_select['ERNAME'].unique())\n",
    "# print('CCSNAME', gdf_select['CCSNAME'].unique())\n",
    "# display(getCityByName(cityname)['gdf'].iloc[0,:])\n",
    "fillCity(getCityByName(cityname), gdf_select, 'CSDNAME', method='equals', names=['Vancouver','Burnaby','Richmond','New Westminster'])\n",
    "mapCitiesAdjacent(getCityByName(cityname),'Area','Canadian Cities, Dissemination Areas colored by square meters',tooltip_DA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next let's examin Halifax, which requires removal of the large sparse areas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cityname = 'Halifax'\n",
    "gdf_select = getCityByName(cityname)['gdf']\n",
    "updateCity(getCityByName(cityname),gdf_select.loc[gdf_select['CTNAME']<123,:])\n",
    "mapCitiesAdjacent(getCityByName(cityname),'CTNAME','Canadian Cities, Dissemination Areas colored by Census Tract ID',tooltip_DA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now review the selected areas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapCitiesAdjacent(getCityGroup('Canada'),'Area','Canadian Cities, Dissemination Areas colored by square meters',tooltip_DA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because the Dissemination Areas are often small, we should also examine Census Tracts and Aggregate Dissemination Areas, which are the next largest reporting area designation.  Geometries for these areas can also be downloaded [here](https://www12.statcan.gc.ca/census-recensement/2011/geo/bound-limit/bound-limit-2016-eng.cfm).\n",
    "\n",
    "We first save our existing work and then examine each grouping in detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cities_DA = cities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate Dissemination Area\n",
    "\n",
    "We collect the processing applied above for Dissemination Areas to see how ADAs compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gdf_CA_ADA = GMLtoGDF(DIR_DATA+'lada000b16g_e.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf_CA_ADA\n",
    "cities = []\n",
    "citygroup = 'Canada'\n",
    "\n",
    "for cityname in citynames_CA:\n",
    "    if not cityname in [city['name'] for city in cities]:\n",
    "        cities.append({'name':cityname, 'group':citygroup})\n",
    "\n",
    "for city in getCityGroup(citygroup):\n",
    "    city_da = []\n",
    "    for c in cities_DA:\n",
    "        if c['name']==city['name']:\n",
    "            city_da = c\n",
    "    fillCity(city,gdf,'ADAUID','equals',city_da['gdf']['ADAUID'].unique().tolist())\n",
    "\n",
    "# Remove two large areas in Halifax\n",
    "updateCity(getCityByName('Halifax'),getCityByName('Halifax')['gdf'].loc[~((getCityByName('Halifax')['gdf']['ADAUID']==12090050) | (getCityByName('Halifax')['gdf']['ADAUID']==12090010)),:])\n",
    "\n",
    "cities_ADA = cities\n",
    "\n",
    "tooltip_ADA = ['PRNAME','CDNAME','ADAUID']\n",
    "mapCitiesAdjacent(getCityGroup(citygroup),'Area','Canadian Cities, Aggregate Dissemination Areas colored by square meters',tooltip_ADA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DA Count per city:')\n",
    "print(count_DA)\n",
    "print('ADA Count per city:')\n",
    "count_ADA = [(city['name'], city['gdf'].shape[0]) for city in cities]\n",
    "print(count_ADA)\n",
    "print('DA/ADA Ratio per city:')\n",
    "print([(a[0],float(f'{a[1]/b[1]:0.3}')) for a, b, in zip(count_DA,count_ADA)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ADAs have a reasonable size - the smallest are about 300 meters across, which is anly a bit smaller than our target walking distance.  The necessary processing was very similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Census Tract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gdf_CA_CT = GMLtoGDF(DIR_DATA+'lct_000b16g_e.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf_CA_CT\n",
    "cities = []\n",
    "citygroup = 'Canada'\n",
    "\n",
    "for cityname in citynames_CA:\n",
    "    if not cityname in [city['name'] for city in cities]:\n",
    "        cities.append({'name':cityname, 'group':citygroup})\n",
    "\n",
    "for city in getCityGroup(citygroup):\n",
    "    city_da = []\n",
    "    for c in cities_DA:\n",
    "        if c['name']==city['name']:\n",
    "            city_da = c\n",
    "    fillCity(city,gdf,'CTUID','equals',city_da['gdf']['CTUID'].unique().tolist())\n",
    "\n",
    "# Remove two large areas in Halifax\n",
    "#updateCity(getCityByName('Halifax'),getCityByName('Halifax')['gdf'].loc[~((tmp_gdf['ADAUID']==12090050) | (tmp_gdf['ADAUID']==12090010)),:])\n",
    "\n",
    "cities_CT = cities\n",
    "\n",
    "tooltip_ADA = ['PRNAME','CMANAME','CTUID']\n",
    "mapCitiesAdjacent(getCityGroup(citygroup),'Area','Canadian Cities, Census Tracts colored by square meters',tooltip_ADA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Sortation Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Forward Sortation Areas are not included in the Dissemination Area boundary file fields, though census data is provided sorted by FSA.  The FSA identifiers do not have detailed geographic correspondence, so we must seek an alternate way to determine what FSAs are of interest (within the bounds of a city).\n",
    "\n",
    "Ideally we could use the Postal Code Correspondence File (PCCF) provided by Canada Post, but this is a restricted document (the PCCF goes into greater detail by linking the full postal code with Dissemination Areas/Blocks, and is therefore a privacy concern).\n",
    "\n",
    "However, Statistics Canada provides boundary files for FSAs, so we can use these in combination with a DA boundary file to create a correspondence list.  The main caveat is that the FSA boundaries are not exactly the Canada Post FSA boundaries, but are derived from self-reported postal codes in census responses, massaged to respect DA and CT boundaries.  This is good for creation of a correspondence list and for consistency of census statistics reporting, but is not strictly accurate.\n",
    "\n",
    "We begin by generating a correspondence list using the available boundary files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For FSAs, there are no features to corresponde to the previous DAs, ADAs, and CTs.  We can resolve this in three ways: finding FSAs with a centroid within an arbitrary distance of any of the centroids of DAs for each city, creating an overlap dataframe using boundary files for FSAs and DAs and using that to select FSAs overlapping any DA in each city, or looking for an existing correspondence file.  The existing Postal Code Correspondence File (PCCF) is [no longer provided](https://www150.statcan.gc.ca/n1/en/catalogue/92-154-X) through Statistics Canada, and is behind logins or paywalls elsewhere online.\n",
    "\n",
    "We proceed with the more exact solution of constructing our own correspondence file.  Luckily, geopandas has an intersection function that is just what we're looking for (I first found the overlay function, but that is over 100 times slower)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gdf_CA_FSA = GMLtoGDF(DIR_DATA+'lfsa000b16g_e.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersectGDF(gdf1, keyfield1, gdf2, keyfield2, verbosity=1):\n",
    "    '''Find the overlap matrix of two geodataframe geometries\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    gdf1: GeoDataFrame (must match crs of gdf2, will be utilized for vectorized overlap calculation)\n",
    "    keyfield1: column name in gdf1 which uniquely identifies each row and will be used to label the results\n",
    "    gdf2: GeoDataFrame (must match crs of gdf1, will be iterated over for overlap calculation)\n",
    "    keyfield2: column name in gdf2 which uniquely identifies each row and will be used to label the results\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    gdf_union: Geodataframe containing columns of nonzero overlap geometries, corresponding gdf1[keyfield1], and corresponding gdf2[keyfield2], where only one value of gdf1[keyfield1] is selected which is the one with maximum overlap area\n",
    "    times: List of execution times for each overlap calculation; len(times)=gdf2.shape[0]\n",
    "    areas: List of pandas Series of overlap areas; len(areas)=gdf2.shape[0], len(areas[i])=gdf1.shape[0]\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    gdf1 and gdf2 must be set to the same crs\n",
    "    Iterates over gdf2, which should have the larger number of rows of {gdf1,gdf2} in order to minimize required memory (assuming geometries are of roughly equal size)\n",
    "    Uses GeoDataFrame.buffer(0) to correct geometries\n",
    "    '''\n",
    "    # Initialize the return variables\n",
    "    gdf_union = geopandas.GeoDataFrame()\n",
    "    times = []\n",
    "    areas = []\n",
    "\n",
    "    # Ensure we are computing with copies of the dataframes (though this should not be necessary as gdf1, gdf2 are never updated)\n",
    "    gdf1 = gdf1.copy(deep=True)\n",
    "    gdf2 = gdf2.copy(deep=True)\n",
    "    \n",
    "    # Create new dataframes to hold buffered geometries\n",
    "    #  Buffered geometries can often prevent topology errors during overlap calculation, at the risk of discarding portions of the geometry\n",
    "    gdf1b = gdf1.copy(deep=True)\n",
    "    gdf2b = gdf2.copy(deep=True)\n",
    "\n",
    "    start_time = time.time()\n",
    "    gdf1b['Geometry'] = gdf1b['Geometry'].buffer(0)\n",
    "    if verbosity>=1: print(f'Polygon conversion for {keyfield1} completed in {time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))}')\n",
    "\n",
    "    start_time = time.time()\n",
    "    gdf2b['Geometry'] = gdf2b['Geometry'].buffer(0)\n",
    "    if verbosity>=1: print(f'Polygon conversion for {keyfield2} completed in {time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))}')\n",
    "\n",
    "    exceptioncount = 0\n",
    "    start_time = time.time()\n",
    "    # TODO: create optimal modulus for verbose display\n",
    "    verbosecount = 1 if verbosity>=2 else extendbounds(gdf2.shape[0],method='Log10')\n",
    "    for i in range(gdf2.shape[0]):\n",
    "        loop_start = time.time()\n",
    "        try:\n",
    "            gdf_tmp = gdf1['Geometry'].intersection(gdf2['Geometry'].iloc[i]) # Attempt using original geometries\n",
    "        except (shapely.errors.TopologicalError): # This sometimes occurs\n",
    "            if verbosity>=2: print(f'Handling exception at index {i}')\n",
    "            exceptioncount += 1\n",
    "            gdf_tmp = gdf1b['Geometry'].intersection(gdf2b['Geometry'].iloc[i]) # Use fallback buffered geometries\n",
    "\n",
    "        areas.append(gdf_tmp.area)\n",
    "        ind = np.argmax(areas[-1]) # The FSA boundaries respect DA boundaries according to the , so there is just one FSA associated with each DA, which should have significantly larger area than any other.\n",
    "        gdf_tmp = geopandas.GeoSeries(gdf_tmp.iloc[ind],crs=gdf1['Geometry'].crs)\n",
    "        gdf_tmp = geopandas.GeoDataFrame(geometry=gdf_tmp,crs=gdf_tmp.crs)\n",
    "        gdf_tmp[keyfield1] = gdf1[keyfield1].iloc[ind]\n",
    "        gdf_tmp[keyfield2] = gdf2[keyfield2].iloc[i]\n",
    "        gdf_union = gdf_union.append(gdf_tmp,ignore_index=True)\n",
    "        loop_end = time.time()\n",
    "        times.append(loop_end-loop_start)\n",
    "        if verbosity>=1 and (i%verbosecount==0 or i+1==gdf2.shape[0]):\n",
    "            print(f'Processed row {i+1}/{gdf2.shape[0]}, {gdf_tmp.shape[0]} overlap found in {loop_end-loop_start:.1f} sec, {gdf_union.shape[0]} overlaps total in {time.strftime(\"%H:%M:%S\", time.gmtime(loop_end-start_time))}, total exceptions {exceptioncount}')\n",
    "    if verbosity>=1: print('Overlap processing complete')\n",
    "    return gdf_union, times, areas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we compute the overlap between FSAs and DAs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try: # Results have been calculated previously, load them to save time\n",
    "    with open(DIR_RESULTS+'GDF_FSA-DA.db','rb') as file:\n",
    "        gdf_union = dill.load(file)\n",
    "    with open(DIR_RESULTS+'GDF_FSA-DA_times.db','rb') as file:\n",
    "        times = dill.load(file)\n",
    "    with open(DIR_RESULTS+'GDF_FSA-DA_areas.db','rb') as file:\n",
    "        areas = dill.load(file)\n",
    "    print('Results loaded from file')\n",
    "\n",
    "except IOError:  # Results not found in file, regenerate them\n",
    "    gdf_union, times, areas = intersectGDF(gdf_CA_FSA,'CFSAUID',gdf_CA_DA,'DAUID',verbosity=1)\n",
    "\n",
    "    with open(DIR_RESULTS+'GDF_FSA-DA.db','wb+') as file:\n",
    "        dill.dump(gdf_union,file)\n",
    "    with open(DIR_RESULTS+'GDF_FSA-DA_times.db','wb+') as file:\n",
    "        dill.dump(times,file)\n",
    "    with open(DIR_RESULTS+'GDF_FSA-DA_areas.db','wb+') as file:\n",
    "        dill.dump(areas,file)\n",
    "    print('Results saved to file')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_union.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dill"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('GDF_FSA-DA.db','wb') as file:\n",
    "    dill.dump(gdf_union,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('GDF_FSA-DA_times.db','wb') as file:\n",
    "    dill.dump(times,file)\n",
    "with open('GDF_FSA-DA_areas.db','wb') as file:\n",
    "    dill.dump(areas,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Do the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(areas)\n",
    "#a.head(20)\n",
    "print(a.shape)\n",
    "print(gdf1.shape[0])\n",
    "print(gdf2.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We look at the results to make sure the selection criteria were reasonable (we check the ratio of the largest to the next largest area and ensure it is arbitrarily large)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(a.head(20))\n",
    "asorted = np.array(areas)\n",
    "asorted.sort(axis=1)\n",
    "display(pd.DataFrame(asorted).head(20))\n",
    "aratio = asorted[:,-2]/asorted[:,-1]\n",
    "aratio[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent = np.logspace(-10,0,11)\n",
    "counts = [len(np.where(aratio>x)[0]) for x in exponent]\n",
    "print('Ratio : Count')\n",
    "print('-------------')\n",
    "for x, c in zip(exponent, counts):\n",
    "    print(x,':',c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a five magnitude gap between tiny ratios where the errors are likely the result of minor coordinate differences, and large ratios indicating that two FSAs share significant overlap with a single DA, which is an apparent violation of the specified [rules]()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rind = np.where(aratio>1e-3)[0]\n",
    "rval = aratio[rind]\n",
    "print('Index : Ratio')\n",
    "print('-------------')\n",
    "for i, v in zip(rind,rval):\n",
    "    print(f'{i: >5}',':',f'{v:.3f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also look at whether the areas match "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the accompanying description file for the FSA boundaries (92-179-g2016001-eng.pdf), it notes that there are \"twenty-one forward sortation areas which are not included in the boundary file because they were not the dominant FSA in a dissemination area.\"  This matching with the number of non-uniquely-associated FSA labels is expected to be a coincidence.\n",
    "\n",
    "To complete the correspondence file, the boundaries involved in the anomalous ratios should be manually examined and corrected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in np.array(areas):\n",
    "    row.sort()\n",
    "    print(np.sort(row)[-2:])\n",
    "    break\n",
    "print(type(areas))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, this process might be easier if waterfronts are avoided, by using digital instead of cartographic boundary files, but it is a little late for that now (but might be useful to keep in mind for future projects)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Operation\n",
    "\n",
    "    Polygon conversion for CFSAUID completed in 00:00:39\n",
    "    Polygon conversion for DAUID completed in 00:00:28\n",
    "    Processed row 1/56589, 1 overlap found in 0.3 sec, 1 overlaps total in 00:00:00, total exceptions 0\n",
    "    Processed row 1001/56589, 1 overlap found in 0.2 sec, 1001 overlaps total in 00:05:02, total exceptions 0\n",
    "    Processed row 2001/56589, 1 overlap found in 0.2 sec, 2001 overlaps total in 00:09:46, total exceptions 0\n",
    "    Processed row 3001/56589, 1 overlap found in 0.3 sec, 3001 overlaps total in 00:14:15, total exceptions 0\n",
    "    Processed row 4001/56589, 1 overlap found in 0.3 sec, 4001 overlaps total in 00:18:41, total exceptions 0\n",
    "    Processed row 5001/56589, 1 overlap found in 0.2 sec, 5001 overlaps total in 00:23:00, total exceptions 0\n",
    "    Processed row 6001/56589, 1 overlap found in 0.2 sec, 6001 overlaps total in 00:27:11, total exceptions 0\n",
    "    Processed row 7001/56589, 1 overlap found in 0.3 sec, 7001 overlaps total in 00:31:21, total exceptions 0\n",
    "    Processed row 8001/56589, 1 overlap found in 0.4 sec, 8001 overlaps total in 00:35:38, total exceptions 0\n",
    "    Processed row 9001/56589, 1 overlap found in 0.2 sec, 9001 overlaps total in 00:39:53, total exceptions 0\n",
    "    Processed row 10001/56589, 1 overlap found in 0.3 sec, 10001 overlaps total in 00:44:14, total exceptions 0\n",
    "    Processed row 11001/56589, 1 overlap found in 0.3 sec, 11001 overlaps total in 00:48:30, total exceptions 0\n",
    "    Processed row 12001/56589, 1 overlap found in 0.3 sec, 12001 overlaps total in 00:52:45, total exceptions 0\n",
    "    Processed row 13001/56589, 1 overlap found in 0.3 sec, 13001 overlaps total in 00:57:00, total exceptions 0\n",
    "    Processed row 14001/56589, 1 overlap found in 0.3 sec, 14001 overlaps total in 01:01:15, total exceptions 0\n",
    "    Processed row 15001/56589, 1 overlap found in 0.2 sec, 15001 overlaps total in 01:05:32, total exceptions 0\n",
    "    Processed row 16001/56589, 1 overlap found in 0.3 sec, 16001 overlaps total in 01:09:49, total exceptions 0\n",
    "    Processed row 17001/56589, 1 overlap found in 0.3 sec, 17001 overlaps total in 01:14:16, total exceptions 0\n",
    "    Processed row 18001/56589, 1 overlap found in 0.2 sec, 18001 overlaps total in 01:18:47, total exceptions 0\n",
    "    Processed row 19001/56589, 1 overlap found in 0.2 sec, 19001 overlaps total in 01:23:40, total exceptions 0\n",
    "    Processed row 20001/56589, 1 overlap found in 0.2 sec, 20001 overlaps total in 01:33:37, total exceptions 0\n",
    "    Processed row 21001/56589, 1 overlap found in 0.3 sec, 21001 overlaps total in 01:37:55, total exceptions 0\n",
    "    Processed row 22001/56589, 1 overlap found in 0.3 sec, 22001 overlaps total in 01:42:12, total exceptions 0\n",
    "    Processed row 23001/56589, 1 overlap found in 0.2 sec, 23001 overlaps total in 01:46:26, total exceptions 0\n",
    "    Processed row 24001/56589, 1 overlap found in 0.3 sec, 24001 overlaps total in 01:50:42, total exceptions 0\n",
    "    Processed row 25001/56589, 1 overlap found in 0.3 sec, 25001 overlaps total in 01:54:55, total exceptions 0\n",
    "    Processed row 26001/56589, 1 overlap found in 0.2 sec, 26001 overlaps total in 01:59:11, total exceptions 0\n",
    "    Processed row 27001/56589, 1 overlap found in 0.2 sec, 27001 overlaps total in 02:03:25, total exceptions 0\n",
    "    Processed row 28001/56589, 1 overlap found in 0.3 sec, 28001 overlaps total in 02:07:38, total exceptions 0\n",
    "    Processed row 29001/56589, 1 overlap found in 0.2 sec, 29001 overlaps total in 02:11:54, total exceptions 0\n",
    "    Processed row 30001/56589, 1 overlap found in 0.3 sec, 30001 overlaps total in 02:16:09, total exceptions 0\n",
    "    Processed row 31001/56589, 1 overlap found in 0.3 sec, 31001 overlaps total in 02:20:25, total exceptions 0\n",
    "    Processed row 32001/56589, 1 overlap found in 0.2 sec, 32001 overlaps total in 02:24:40, total exceptions 0\n",
    "    Processed row 33001/56589, 1 overlap found in 0.3 sec, 33001 overlaps total in 02:29:04, total exceptions 0\n",
    "    Processed row 34001/56589, 1 overlap found in 0.3 sec, 34001 overlaps total in 02:33:23, total exceptions 0\n",
    "    Processed row 35001/56589, 1 overlap found in 0.2 sec, 35001 overlaps total in 02:37:43, total exceptions 0\n",
    "    Processed row 36001/56589, 1 overlap found in 0.3 sec, 36001 overlaps total in 02:42:11, total exceptions 0\n",
    "    Processed row 37001/56589, 1 overlap found in 0.3 sec, 37001 overlaps total in 02:46:40, total exceptions 0\n",
    "    Processed row 38001/56589, 1 overlap found in 0.4 sec, 38001 overlaps total in 02:51:08, total exceptions 0\n",
    "    Processed row 39001/56589, 1 overlap found in 0.3 sec, 39001 overlaps total in 02:56:51, total exceptions 0\n",
    "    Processed row 40001/56589, 1 overlap found in 0.3 sec, 40001 overlaps total in 03:01:38, total exceptions 0\n",
    "    Processed row 41001/56589, 1 overlap found in 0.3 sec, 41001 overlaps total in 03:06:38, total exceptions 0\n",
    "    Processed row 42001/56589, 1 overlap found in 0.3 sec, 42001 overlaps total in 03:11:34, total exceptions 0\n",
    "    Processed row 43001/56589, 1 overlap found in 0.2 sec, 43001 overlaps total in 03:16:47, total exceptions 0\n",
    "    Processed row 44001/56589, 1 overlap found in 0.3 sec, 44001 overlaps total in 03:21:20, total exceptions 0\n",
    "    Processed row 45001/56589, 1 overlap found in 0.2 sec, 45001 overlaps total in 03:25:50, total exceptions 0\n",
    "    Processed row 46001/56589, 1 overlap found in 0.3 sec, 46001 overlaps total in 03:30:20, total exceptions 0\n",
    "    Processed row 47001/56589, 1 overlap found in 0.2 sec, 47001 overlaps total in 03:34:45, total exceptions 0\n",
    "    Processed row 48001/56589, 1 overlap found in 0.3 sec, 48001 overlaps total in 03:39:02, total exceptions 0\n",
    "    Processed row 49001/56589, 1 overlap found in 0.3 sec, 49001 overlaps total in 03:43:24, total exceptions 0\n",
    "    Processed row 50001/56589, 1 overlap found in 0.2 sec, 50001 overlaps total in 03:47:42, total exceptions 0\n",
    "    Processed row 51001/56589, 1 overlap found in 0.3 sec, 51001 overlaps total in 03:52:01, total exceptions 0\n",
    "    Processed row 52001/56589, 1 overlap found in 0.3 sec, 52001 overlaps total in 03:56:22, total exceptions 0\n",
    "    Processed row 53001/56589, 1 overlap found in 0.3 sec, 53001 overlaps total in 04:00:44, total exceptions 0\n",
    "    Processed row 54001/56589, 1 overlap found in 0.3 sec, 54001 overlaps total in 04:05:03, total exceptions 0\n",
    "    Processed row 55001/56589, 1 overlap found in 0.2 sec, 55001 overlaps total in 04:21:02, total exceptions 0\n",
    "    Processed row 56001/56589, 1 overlap found in 0.4 sec, 56001 overlaps total in 08:05:59, total exceptions 0\n",
    "    Processed row 56589/56589, 1 overlap found in 0.3 sec, 56589 overlaps total in 17:16:55, total exceptions 0\n",
    "    Overlap processing complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf = gdf_CA_FSA\n",
    "cities = []\n",
    "citygroup = 'Canada'\n",
    "\n",
    "gdf.merge(gdf_union,on='DAUID')\n",
    "\n",
    "for cityname in citynames_CA:\n",
    "    if not cityname in [city['name'] for city in cities]:\n",
    "        cities.append({'name':cityname, 'group':citygroup})\n",
    "\n",
    "for city in getCityGroup(citygroup):\n",
    "    city_da = []\n",
    "    for c in cities_DA:\n",
    "        if c['name']==city['name']:\n",
    "            city_da = c\n",
    "    updateCity(city, gdf_CA_FSA.loc[,:])\n",
    "    fillCity(city,gdf,'CTUID','equals',city_da['gdf']['DAUID'].unique().tolist())\n",
    "\n",
    "# Remove two large areas in Halifax\n",
    "#updateCity(getCityByName('Halifax'),getCityByName('Halifax')['gdf'].loc[~((tmp_gdf['ADAUID']==12090050) | (tmp_gdf['ADAUID']==12090010)),:])\n",
    "\n",
    "cities_FSA = cities\n",
    "\n",
    "tooltip_FSA = ['PRNAME','CMANAME','CTUID']\n",
    "mapCitiesAdjacent(getCityGroup(citygroup),'Area','Canadian Cities, Census Tracts colored by square meters',tooltip_FSA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf11 = gdf_CA_FSA.copy(deep=True)\n",
    "gdf22 = gdf_CA_DA.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio1 = gdf1['Geometry'].area/gdf11['Geometry'].area - 1\n",
    "ratio1.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio2 = gdf2['Geometry'].area/gdf22['Geometry'].area - 1\n",
    "ratio2.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio2df = gdf2[['Geometry']].area/gdf22[['Geometry']].area - 1\n",
    "ratio2df.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio2short = ratio2.loc[abs(ratio2)>1e-6]\n",
    "ratio2short"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(gdf2.loc[50956,'Geometry']))\n",
    "print(len(gdf2.loc[50956,'Geometry'].to_wkt()))\n",
    "len1 = [len(g.to_wkt()) for g in gdf1['Geometry']]\n",
    "len2 = [len(g.to_wkt()) for g in gdf2['Geometry']]\n",
    "len11 = [len(g.to_wkt()) for g in gdf11['Geometry']]\n",
    "len22 = [len(g.to_wkt()) for g in gdf22['Geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statistics\n",
    "print(statistics.mean(len1))\n",
    "print(statistics.mean(len2))\n",
    "print(max(len1))\n",
    "print(max(len2))\n",
    "print(len2[50956])\n",
    "print()\n",
    "print(np.array(times)[np.where(np.array(times)>10)[0]])\n",
    "#print(times[np.where(np.array(times)>10)[:,0]])\n",
    "#print(pd.Series(times).loc[np.argwhere(pd.Series(times)>1)])\n",
    "#plt.plot(times,len2,'.')\n",
    "#plt.xlabel('Time [s]')\n",
    "#plt.ylabel('Length [char]')\n",
    "#plt.xlim([0,100])\n",
    "#plt.ylim([1,10000000])\n",
    "plt.plot(range(len(times)),np.log10([t for t, l in zip(times,len2)]),'.')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Log10(Time)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the difference in geometry between original and .buffer(0) versions through the length of the geometry entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len1 = [len(g.to_wkt()) for g in gdf1['Geometry']]\n",
    "len2 = [len(g.to_wkt()) for g in gdf2['Geometry']]\n",
    "len1b = [len(g.to_wkt()) for g in gdf1b['Geometry']]\n",
    "len2b = [len(g.to_wkt()) for g in gdf2b['Geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ratio1 = np.array(len1)/len1b\n",
    "len_ratio2 = np.array(len2)/len2b\n",
    "plt.plot(len_ratio1)\n",
    "plt.plot(len_ratio2)\n",
    "plt.ylabel('Original:Buffered WKT Length')\n",
    "plt.xlabel('Element Index')\n",
    "print(f'There are {sum(len_ratio1<1)} FSAs with increased geometries, {sum(len_ratio1>1)} with reduced geometries')\n",
    "print(f'There are {sum(len_ratio2<1)} DAs with increased geometries, {sum(len_ratio2>1)} with reduced geometries')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio2.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New code here for FSA correspondence using digital boundary files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gdf_CA_FSA_D = GMLtoGDF('lfsa000a16g_e.gml')\n",
    "gdf_CA_DA_D = GMLtoGDF('lda_000a16g_e.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('shapely.geos').setLevel(logging.CRITICAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf1 = gdf_CA_FSA_D.copy(deep=True)\n",
    "gdf1b = gdf1.copy(deep=True)\n",
    "keyfield1 = 'CFSAUID'\n",
    "gdf2 = gdf_CA_DA_D.copy(deep=True)\n",
    "gdf2b = gdf2.copy(deep=True)\n",
    "keyfield2 = 'DAUID'\n",
    "\n",
    "gdf_union = geopandas.GeoDataFrame()\n",
    "times = []\n",
    "areas = []\n",
    "\n",
    "from shapely.geometry import shape, mapping\n",
    "\n",
    "start_time = time.time()\n",
    "gdf1b['Geometry'] = gdf1b['Geometry'].buffer(0)\n",
    "print(f'Polygon conversion for {keyfield1} completed in {time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))}')\n",
    "\n",
    "start_time = time.time()\n",
    "gdf2b['Geometry'] = gdf2b['Geometry'].buffer(0)\n",
    "print(f'Polygon conversion for {keyfield2} completed in {time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))}')\n",
    "\n",
    "exceptioncount = 0\n",
    "start_time = time.time()\n",
    "for i in range(gdf2.shape[0]):\n",
    "    loop_start = time.time()\n",
    "    try:\n",
    "        gdf_tmp = gdf1['Geometry'].intersection(gdf2['Geometry'].iloc[i]) # This takes about 0.3 seconds per execution\n",
    "    except (shapely.errors.TopologicalError): # This sometimes occurs\n",
    "        #print(f'Handling exception at index {i}')\n",
    "        exceptioncount += 1\n",
    "        gdf_tmp = gdf1b['Geometry'].intersection(gdf2b['Geometry'].iloc[i]) # This may corrupt the area but usually works as fast as the original and reliably does not fail\n",
    "\n",
    "    areas.append(gdf_tmp.area)\n",
    "    ind = np.argmax(areas[-1]) # The FSA boundaries respect DA boundaries according to the , so there is just one FSA associated with each DA, which should have significantly larger area than any other.\n",
    "    gdf_tmp = geopandas.GeoSeries(gdf_tmp.iloc[ind],crs=gdf1['Geometry'].crs)\n",
    "    gdf_tmp = geopandas.GeoDataFrame(geometry=gdf_tmp,crs=gdf_tmp.crs)\n",
    "    gdf_tmp[keyfield1] = gdf1[keyfield1].iloc[ind]\n",
    "    gdf_tmp[keyfield2] = gdf2[keyfield2].iloc[i]\n",
    "    gdf_union = gdf_union.append(gdf_tmp,ignore_index=True)\n",
    "    loop_end = time.time()\n",
    "    times.append(loop_end-loop_start)\n",
    "    if i%1000==0 or i+1==gdf2.shape[0]:\n",
    "        print(f'Processed row {i+1}/{gdf2.shape[0]}, {gdf_tmp.shape[0]} overlap found in {loop_end-loop_start:.1f} sec, {gdf_union.shape[0]} overlaps total in {time.strftime(\"%H:%M:%S\", time.gmtime(loop_end-start_time))}, total exceptions {exceptioncount}')\n",
    "print('Overlap processing complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Polygon conversion for CFSAUID completed in 00:00:01\n",
    "Polygon conversion for DAUID completed in 00:00:04\n",
    "Processed row 1/56590, 1 overlap found in 0.1 sec, 1 overlaps total in 00:00:00, total exceptions 0\n",
    "Processed row 1001/56590, 1 overlap found in 0.1 sec, 1001 overlaps total in 00:02:19, total exceptions 13\n",
    "Processed row 2001/56590, 1 overlap found in 0.2 sec, 2001 overlaps total in 00:04:54, total exceptions 17\n",
    "Processed row 3001/56590, 1 overlap found in 0.1 sec, 3001 overlaps total in 00:07:22, total exceptions 17\n",
    "Processed row 4001/56590, 1 overlap found in 0.1 sec, 4001 overlaps total in 00:09:39, total exceptions 17\n",
    "Processed row 5001/56590, 1 overlap found in 0.1 sec, 5001 overlaps total in 00:11:37, total exceptions 35\n",
    "Processed row 6001/56590, 1 overlap found in 0.1 sec, 6001 overlaps total in 00:13:30, total exceptions 111\n",
    "Processed row 7001/56590, 1 overlap found in 0.1 sec, 7001 overlaps total in 00:15:22, total exceptions 171\n",
    "Processed row 8001/56590, 1 overlap found in 0.1 sec, 8001 overlaps total in 00:17:20, total exceptions 171\n",
    "Processed row 9001/56590, 1 overlap found in 0.1 sec, 9001 overlaps total in 00:19:15, total exceptions 207\n",
    "Processed row 10001/56590, 1 overlap found in 0.1 sec, 10001 overlaps total in 00:21:07, total exceptions 295\n",
    "Processed row 11001/56590, 1 overlap found in 0.1 sec, 11001 overlaps total in 00:23:00, total exceptions 472\n",
    "Processed row 12001/56590, 1 overlap found in 0.1 sec, 12001 overlaps total in 00:24:52, total exceptions 472\n",
    "Processed row 13001/56590, 1 overlap found in 0.1 sec, 13001 overlaps total in 00:26:34, total exceptions 472\n",
    "Processed row 14001/56590, 1 overlap found in 0.1 sec, 14001 overlaps total in 00:28:22, total exceptions 472\n",
    "Processed row 15001/56590, 1 overlap found in 0.1 sec, 15001 overlaps total in 00:30:08, total exceptions 472\n",
    "Processed row 16001/56590, 1 overlap found in 0.1 sec, 16001 overlaps total in 00:31:59, total exceptions 473\n",
    "Processed row 17001/56590, 1 overlap found in 0.1 sec, 17001 overlaps total in 00:33:53, total exceptions 474\n",
    "Processed row 18001/56590, 1 overlap found in 0.1 sec, 18001 overlaps total in 00:35:58, total exceptions 475\n",
    "Processed row 19001/56590, 1 overlap found in 0.1 sec, 19001 overlaps total in 00:37:56, total exceptions 476\n",
    "Processed row 20001/56590, 1 overlap found in 0.1 sec, 20001 overlaps total in 00:39:46, total exceptions 476\n",
    "Processed row 21001/56590, 1 overlap found in 0.1 sec, 21001 overlaps total in 00:41:31, total exceptions 476\n",
    "Processed row 22001/56590, 1 overlap found in 0.2 sec, 22001 overlaps total in 00:43:20, total exceptions 477\n",
    "Processed row 23001/56590, 1 overlap found in 0.1 sec, 23001 overlaps total in 00:45:09, total exceptions 477\n",
    "Processed row 24001/56590, 1 overlap found in 0.1 sec, 24001 overlaps total in 00:46:58, total exceptions 477\n",
    "Processed row 25001/56590, 1 overlap found in 0.1 sec, 25001 overlaps total in 00:48:45, total exceptions 477\n",
    "Processed row 26001/56590, 1 overlap found in 0.1 sec, 26001 overlaps total in 00:50:33, total exceptions 477\n",
    "Processed row 27001/56590, 1 overlap found in 0.1 sec, 27001 overlaps total in 00:52:22, total exceptions 478\n",
    "Processed row 28001/56590, 1 overlap found in 0.1 sec, 28001 overlaps total in 00:54:09, total exceptions 479\n",
    "Processed row 29001/56590, 1 overlap found in 0.1 sec, 29001 overlaps total in 00:55:56, total exceptions 479\n",
    "Processed row 30001/56590, 1 overlap found in 0.1 sec, 30001 overlaps total in 00:57:42, total exceptions 479\n",
    "Processed row 31001/56590, 1 overlap found in 0.1 sec, 31001 overlaps total in 00:59:31, total exceptions 479\n",
    "Processed row 32001/56590, 1 overlap found in 0.1 sec, 32001 overlaps total in 01:01:25, total exceptions 479\n",
    "Processed row 33001/56590, 1 overlap found in 0.1 sec, 33001 overlaps total in 01:03:15, total exceptions 480\n",
    "Processed row 34001/56590, 1 overlap found in 0.1 sec, 34001 overlaps total in 01:05:06, total exceptions 480\n",
    "Processed row 35001/56590, 1 overlap found in 0.1 sec, 35001 overlaps total in 01:07:06, total exceptions 481\n",
    "Processed row 36001/56590, 1 overlap found in 0.1 sec, 36001 overlaps total in 01:09:03, total exceptions 484\n",
    "Processed row 37001/56590, 1 overlap found in 0.1 sec, 37001 overlaps total in 01:11:06, total exceptions 517\n",
    "Processed row 38001/56590, 1 overlap found in 0.1 sec, 38001 overlaps total in 01:13:13, total exceptions 607\n",
    "Processed row 39001/56590, 1 overlap found in 0.1 sec, 39001 overlaps total in 01:15:33, total exceptions 747\n",
    "Processed row 40001/56590, 1 overlap found in 0.2 sec, 40001 overlaps total in 01:18:16, total exceptions 876\n",
    "Processed row 41001/56590, 1 overlap found in 0.1 sec, 41001 overlaps total in 01:21:24, total exceptions 1118\n",
    "Processed row 42001/56590, 1 overlap found in 0.1 sec, 42001 overlaps total in 01:24:29, total exceptions 1190\n",
    "Processed row 43001/56590, 1 overlap found in 0.1 sec, 43001 overlaps total in 01:27:36, total exceptions 1236\n",
    "Processed row 44001/56590, 1 overlap found in 0.1 sec, 44001 overlaps total in 01:30:12, total exceptions 1320\n",
    "Processed row 45001/56590, 1 overlap found in 0.1 sec, 45001 overlaps total in 01:32:26, total exceptions 1354\n",
    "Processed row 46001/56590, 1 overlap found in 0.2 sec, 46001 overlaps total in 01:34:42, total exceptions 1355\n",
    "Processed row 47001/56590, 1 overlap found in 0.1 sec, 47001 overlaps total in 01:37:05, total exceptions 1356\n",
    "Processed row 48001/56590, 1 overlap found in 0.1 sec, 48001 overlaps total in 01:39:18, total exceptions 1359\n",
    "Processed row 49001/56590, 1 overlap found in 0.2 sec, 49001 overlaps total in 01:41:24, total exceptions 1361\n",
    "Processed row 50001/56590, 1 overlap found in 0.1 sec, 50001 overlaps total in 01:43:32, total exceptions 1364\n",
    "Processed row 51001/56590, 1 overlap found in 0.1 sec, 51001 overlaps total in 01:45:23, total exceptions 1364\n",
    "Processed row 52001/56590, 1 overlap found in 0.1 sec, 52001 overlaps total in 01:47:14, total exceptions 1364\n",
    "Processed row 53001/56590, 1 overlap found in 0.1 sec, 53001 overlaps total in 01:49:13, total exceptions 1364\n",
    "Processed row 54001/56590, 1 overlap found in 0.1 sec, 54001 overlaps total in 01:51:15, total exceptions 1365\n",
    "Processed row 55001/56590, 1 overlap found in 0.1 sec, 55001 overlaps total in 01:53:22, total exceptions 1365\n",
    "Processed row 56001/56590, 1 overlap found in 0.1 sec, 56001 overlaps total in 01:55:39, total exceptions 1366\n",
    "Processed row 56590/56590, 1 overlap found in 0.3 sec, 56590 overlaps total in 01:57:35, total exceptions 1368\n",
    "Overlap processing complete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dill\n",
    "with open('GDF_FSA-DA-D.db','wb') as file:\n",
    "    dill.dump(gdf_union,file)\n",
    "with open('GDF_FSA-DA-D_times.db','wb') as file:\n",
    "    dill.dump(times,file)\n",
    "with open('GDF_FSA-DA-D_areas.db','wb') as file:\n",
    "    dill.dump(areas,file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import dill\n",
    "with open('GDF_FSA-DA-D.db','r') as file:\n",
    "    gdf_union = dill.load(file)\n",
    "with open('GDF_FSA-DA-D_times.db','r') as file:\n",
    "    times = dill.load(file)\n",
    "with open('GDF_FSA-DA-D_areas.db','r') as file:\n",
    "    areas = dill.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Processing overlap with digital as opposed to geographic boundary files goes generally three times faster (probably due to many island and river boundaries in the latter), and all segments seem to take around 2 minutes, indicating no significant anomalous delays.  Let's see if there are any processing time outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(times)),np.log10([t for t, l in zip(times,len2)]),'.')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Log$_{10}$(Time [s])');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can look at the difference in geometry between original and .buffer(0) versions through the length of the geometry entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len1 = [len(g.to_wkt()) for g in gdf1['Geometry']]\n",
    "len2 = [len(g.to_wkt()) for g in gdf2['Geometry']]\n",
    "len1b = [len(g.to_wkt()) for g in gdf1b['Geometry']]\n",
    "len2b = [len(g.to_wkt()) for g in gdf2b['Geometry']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_ratio1 = np.array(len1)/len1b\n",
    "len_ratio2 = np.array(len2)/len2b\n",
    "plt.plot(len_ratio1)\n",
    "plt.plot(len_ratio2)\n",
    "plt.ylabel('Original:Buffered WKT Length')\n",
    "plt.xlabel('Element Index')\n",
    "print(f'There are {len(len_ratio1)} FSAs with {sum(len_ratio1<1)} increased geometries and {sum(len_ratio1>1)} reduced geometries')\n",
    "print(f'There are {len(len_ratio2)} DAs with {sum(len_ratio2<1)} increased geometries and {sum(len_ratio2>1)} reduced geometries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This indicates minor differences; even with 1368 exceptions (out of 56590 DAs) during unbuffered geometry processing, these (likely) arise from only 9 FSAs and 53 DAs where the number of points in their geometry changes upon buffering.\n",
    "\n",
    "We can also look at how the area of the geometries changes upon buffering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio1 = gdf1b['Geometry'].area/gdf1['Geometry'].area - 1\n",
    "print(f'There are {sum(ratio1!=0)} indices where areas differ, with {len(ratio1.unique())-1} unique nonzero area difference fraction values.\\nNonzero differences presented below, where area fraction = A(original)/A(buffered) - 1 :\\nRatio : Area Frac. : Multiples\\n------|------------|----------')\n",
    "for i, v, m in zip(np.where(ratio1!=0)[0],ratio1[ratio1!=0],ratio1[ratio1!=0]/min(abs(ratio1[ratio1!=0]))):\n",
    "    print(f'{i:>5} : {v: .3e} : {int(m): >3d}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio2 = gdf2b['Geometry'].area/gdf2['Geometry'].area - 1\n",
    "print(f'There are {sum(ratio2!=0)} indices where areas differ, with {len(ratio2.unique())-1} unique nonzero area difference fraction values.\\nNonzero differences presented below, where area fraction = A(original)/A(buffered) - 1 :\\nRatio : Area Frac. : Multiples\\n------|------------|----------')\n",
    "for i, v, m in zip(np.where(ratio2!=0)[0],ratio2[ratio2!=0],ratio2[ratio2!=0]/min(abs(ratio2[ratio2!=0]))):\n",
    "    print(f'{i:>5} : {v: .3e} : {int(m): >3d}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These FSA area deviations are the same order of magnitude as the DA area deviations, within an order of magnitude of the area quantum.  There are 13 digits of precision in the GML file, but I have not yet investigated the precision of GeoDataFrames or Geopandas internal calculations (though note that buffer has a default resolution of 16, perhaps the area calculatio does also).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All of the above are very small area differences, compared to the ~1e-5 anomalies seen when using the geographic boundary files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = pd.DataFrame(areas)\n",
    "display(a.head(3))\n",
    "print(a.shape)\n",
    "print(gdf1.shape[0])\n",
    "print(gdf2.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#display(a.head(20))\n",
    "asorted = np.array(areas)\n",
    "asorted.sort(axis=1)\n",
    "#display(pd.DataFrame(asorted).head(20))\n",
    "aratio = asorted[:,-2]/asorted[:,-1]\n",
    "#aratio[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent = np.logspace(-10,0,11)\n",
    "counts = [len(np.where(aratio>x)[0]) for x in exponent]\n",
    "print('Ratio : Count')\n",
    "print('-------------')\n",
    "for x, c in zip(exponent, counts):\n",
    "    print(x,':',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exponent = np.logspace(-10,0,11)\n",
    "counts = [len(np.where(areas[:]>x)[0]) for x in exponent]\n",
    "print('Area  : Count')\n",
    "print('------|------')\n",
    "for x, c in zip(exponent, counts):\n",
    "    print(x,':',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rind = np.where(aratio>1e-3)[0]\n",
    "rval = aratio[rind]\n",
    "print('Index : Ratio')\n",
    "print('-------------')\n",
    "for i, v in zip(rind,rval):\n",
    "    print(f'{i: >5}',':',f'{v:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf_CA_DA_D.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at one of the suspect geometries:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = np.where(areas[geom_index]>1e-10)[0]\n",
    "print(ind)\n",
    "print(areas[geom_index][ind])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So use ratio to determine cutoff for indices for DA\n",
    "Plot da's with area as a thing\n",
    "\n",
    "function to get DAs from a FSA\n",
    "function to get FSAs from a DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "geom_index = 53335\n",
    "\n",
    "fsa = gdf_union.loc[geom_index,'CFSAUID']\n",
    "das = areas[geom_index]\n",
    "\n",
    "# create a plain world map\n",
    "geom_centroid = gdf_union.iloc[geom_index,:].geometry.centroid\n",
    "geom_centroid = [geom_centroid.y,geom_centroid.x]\n",
    "\n",
    "geom_map = folium.Map(location=geom_centroid, control_scale=True)\n",
    "#geom_map.fit_bounds(cities[city_index]['bounds'])\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=gdf_union.iloc[[geom_index]].to_json(),\n",
    "    data=gdf_union,\n",
    "    columns=['CFSAUID', 'DAUID'],\n",
    "    key_on='feature.properties.CFSAUID',\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    ").add_to(geom_map)\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=gdf_CA_DA_D.iloc[[geom_index]].to_json(),\n",
    "    data=gdf_union,\n",
    "    columns=['CFSAUID', 'DAUID'],\n",
    "    key_on='feature.properties.DAUID',\n",
    "    fill_color='GnBu', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    ").add_to(geom_map)\n",
    "\n",
    "# display map\n",
    "print(f'FSA: {fsa}')\n",
    "display(geom_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Old code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "gdf_CA_FSA = GMLtoGDF('lfsa000b16g_e.gml')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison of Areas\n",
    "\n",
    "Let's compare some metrics for the various ways of dividing up the regions of interest.\n",
    "\n",
    "First, let's look at the total number of regions for each dividing method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DA Count per city:')\n",
    "print(count_DA)\n",
    "print('ADA Count per city:')\n",
    "print(count_ADA)\n",
    "print('CT Count per city:')\n",
    "count_CT = [(city['name'], city['gdf'].shape[0]) for city in cities]\n",
    "print(count_ADA)\n",
    "print('DA/CT Ratio per city:')\n",
    "print([(a[0],float(f'{a[1]/b[1]:0.3}')) for a, b, in zip(count_DA,count_CT)])\n",
    "print('ADA/CT Ratio per city:')\n",
    "print([(a[0],float(f'{a[1]/b[1]:0.3}')) for a, b, in zip(count_ADA,count_CT)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are roughly twice as many Census Tracts as Aggregate Dissemination Areas in each city, with a corresponding smaller area.\n",
    "\n",
    "We can also look at the areas occupied by each division.  The target area is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radius = 250\n",
    "a = np.pi*radius**2\n",
    "print(f'For radius {radius} meters, target area is {a:.0f} square meters (log10(area) = {np.log10(a):0.3})')\n",
    "radius = 500\n",
    "a = np.pi*radius**2\n",
    "print(f'For radius {radius} meters, target area is {a:.0f} square meters (log10(area) = {np.log10(a):0.3})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def histCities(cities,featurename,regiontype=None):\n",
    "    if type(cities)==dict:\n",
    "        cities = [cities]\n",
    "    if type(regiontype)==str:\n",
    "        regiontype=[regiontype]*len(cities)\n",
    "    n = len(cities)\n",
    "    plt.figure(figsize=(4*n,3))\n",
    "    bounds = extendBounds([0, (getCityBounds(cities,featurename))[1]])\n",
    "    for i, city in enumerate(cities):\n",
    "        plt.subplot(1,n,i+1)\n",
    "        plt.hist(city['gdf'][featurename],bins=np.arange(0,bounds[1],extendBounds([0, bounds[1]/10])[1]))\n",
    "        plt.xlabel(featurename)\n",
    "        plt.ylabel(f'Number of {\"Regions\" if regiontype==None else regiontype[i]}')\n",
    "        plt.title(f\"{city['name']}, {city['gdf'].shape[0]} {'Regions' if regiontype==None else regiontype[i]}\")\n",
    "        plt.xticks(np.arange(0,bounds[1],extendBounds([0, bounds[1]/10])[1]))\n",
    "\n",
    "def histCitiesLog10(cities,featurename,regiontype=None):\n",
    "    if type(cities)==dict:\n",
    "        cities = [cities]\n",
    "    if type(regiontype)==str:\n",
    "        regiontype=[regiontype]*len(cities)\n",
    "    n = len(cities)\n",
    "    plt.figure(figsize=(4*n,4))\n",
    "    bounds = np.log10(extendBounds(getCityBounds(cities,featurename),'nearestPower',10))\n",
    "    span = bounds[1]-bounds[0]\n",
    "    step = 1\n",
    "    labelstep = 1\n",
    "    while span/step<10:\n",
    "        if np.log10(step)%1==0: # leading digit is 1\n",
    "            labelstep = step\n",
    "            step=step/2\n",
    "        elif np.log10(step)%1>0.5: # leading digit is 5\n",
    "            labelstep = step\n",
    "            step=step*2/5\n",
    "        else: # leading digit is 2\n",
    "            labelstep = step\n",
    "            step=step/2\n",
    "    width = 0.8*step\n",
    "    for i, city in enumerate(cities):\n",
    "        plt.subplot(1,n,i+1)\n",
    "        plt.hist(city['gdf'][featurename].apply(np.log10),bins=np.arange(bounds[0]-width/2,bounds[1]+width/2,step),width=0.8*step)\n",
    "        plt.xlabel(f'Log$_{{{10}}}$({featurename})')\n",
    "        plt.ylabel(f'Number of {\"Regions\" if regiontype==None else regiontype[i]}')\n",
    "        plt.title(f\"{city['name']}, {city['gdf'].shape[0]} {'Regions' if regiontype==None else regiontype[i]}\")\n",
    "        plt.xticks(np.arange(bounds[0],bounds[1],labelstep))\n",
    "        plt.gca().add_patch(plt.Rectangle((5.29,0), 0.61, plt.ylim()[1], color='red', alpha=0.3))\n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now plot the distribution of areas for each city when considering different area types.  The red area is bounded by areas corresponding to circular areas with diameter of 500 meters and radius of 500 meters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histCitiesLog10(cities_DA,'Area','Dissemination Areas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histCitiesLog10(cities_ADA,'Area','Agg. Dissemination Areas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histCitiesLog10(cities_CT,'Area','Census Tracts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "histCitiesLog10(cities_FSA,'Area','Forward Sortation Areas')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see quantitatively that CTs best match our target area range, while ADAs are larger and DAs smaller."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Census Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File T1901EN.CSV was downloaded from the [Canadian Census Data](https://www12.statcan.gc.ca/census-recensement/2016/dp-pd/hlt-fst/pd-pl/comprehensive.cfm) site"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "## Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Osgeo package\n",
    "\n",
    "Before finding the geopandas library, I was using [gdal](https://gdal.org/python/index.html) (which is one of many dependencies of geopandas).  Using gdal's ogr and osr modules in the osgeo package I read in the gml file, converted coordinates to accurately compute area, and converted coordinates again to latitude and longitude.  As a guide to learning the library I adapted code from some [examples](https://pcjericks.github.io/py-gdalogr-cookbook/geometry.html#quarter-polygon-and-create-centroids).  Conversion to geojson may be done explicitly as demonstrated [here](https://gis.stackexchange.com/questions/77974/converting-gml-to-geojson-using-python-and-ogr-with-geometry-transformation), but by the time I was going to convert to geojson I found it much easier with geopandas, and this approach was abandoned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parsing GML file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from osgeo import ogr\n",
    "    from osgeo import osr\n",
    "    \n",
    "    def parseGMLtoDF(fn_gml,limit=None):\n",
    "        '''Reads in a GML file and outputs a DataFrame\n",
    "\n",
    "        Adds columns for Geometry (in WKT format), Latitude (of centroid), Longitude (of centroid) and Area (in square meters)\n",
    "        '''\n",
    "        # Add lightweight progress bar, source copied from GitHub\n",
    "        import ipypb\n",
    "\n",
    "        # Read in the file\n",
    "        source = ogr.Open(fn_gml)\n",
    "        layer = source.GetLayer(0) # there is only one Layer (the FeatureCollection)\n",
    "\n",
    "        # Get a list of field names to extract (also could be gotten from schema e.g. lda_000b16g_e.xsd)\n",
    "        #   Fields are in order (sequence) and none are required (minOccurs=0)\n",
    "        layerDefinition = layer.GetLayerDefn()\n",
    "        layerFields = [layerDefinition.GetFieldDefn(i).GetName() for i in range(layerDefinition.GetFieldCount())]\n",
    "\n",
    "        # Initialize the output dataframe\n",
    "        df = pd.DataFrame(columns=[*layerFields, 'Geometry', 'Latitude', 'Longitude', 'Area'])\n",
    "\n",
    "        # Extract data from the features\n",
    "        parse_limit = layer.GetFeatureCount()\n",
    "        if not limit is None:\n",
    "            parse_limit = limit\n",
    "\n",
    "        for i in ipypb.track(range(parse_limit)):\n",
    "            # Get the feature to be processed\n",
    "            feature = layer.GetNextFeature()\n",
    "\n",
    "            # Copy all fields into an empty dataframe\n",
    "            df_tmp = pd.DataFrame(columns=df.columns)\n",
    "\n",
    "            # Copy all fields individually, in case some are missing\n",
    "            for i, fieldname in enumerate(layerFields):\n",
    "                if feature.IsFieldSet(i):\n",
    "                    df_tmp.loc[0,fieldname] = feature.GetFieldAsString(fieldname)\n",
    "\n",
    "            # Get the latitude and longitude\n",
    "            inref = feature.GetGeometryRef().GetSpatialReference() # EPSG 3347\n",
    "            llref = osr.SpatialReference()\n",
    "            llref.ImportFromEPSG(4326) # coordinates for this geometry are latitude, longitude\n",
    "            lltransform = osr.CoordinateTransformation(inref, llref)\n",
    "            geom = feature.GetGeometryRef().Clone()\n",
    "            geom.Transform(lltransform)\n",
    "            centroid = geom.Centroid()\n",
    "            df_tmp['Latitude'] = centroid.GetX()\n",
    "            df_tmp['Longitude'] = centroid.GetY()\n",
    "            df_tmp['Geometry'] = geom.ExportToWkt()\n",
    "\n",
    "            # Get the area by converting to a locally-appropriate coordinate system\n",
    "            inref = feature.GetGeometryRef().GetSpatialReference()\n",
    "            arref = osr.SpatialReference()\n",
    "            arref.ImportFromEPSG(6931) # TODO: let this choose appropriate projection based on centroid for full-globe applicability, or alter to be a geodesic calculation.  Lambert Cylindrical Equal Area 6933, U.S. National Atlas Equal Area Projection 2163, NSIDC EASE-Grid North 3408, WGS 84 / NSIDC EASE-Grid 2.0 North 6931, WGS 84 / NSIDC EASE-Grid 2.0 South 6932, WGS 84 / NSIDC EASE-Grid 2.0 Global/Temperate 6933\n",
    "            artransform = osr.CoordinateTransformation(inref, arref)\n",
    "            geom = feature.GetGeometryRef().Clone() # Fresh instance to prevent error accumulation from multiple transforms\n",
    "            geom.Transform(artransform)\n",
    "            df_tmp['Area'] = geom.Area() # TODO: investigate proj module, related to osgeo, for geodesic area\n",
    "\n",
    "            # Add the feature dataframe to the output\n",
    "            df = df.append(df_tmp, ignore_index=True)\n",
    "\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Viewing fields\n",
    "\n",
    "    layerDefinition = s2.GetLayerDefn()\n",
    "\n",
    "    for i in range(layerDefinition.GetFieldCount()):\n",
    "        print(layerDefinition.GetFieldDefn(i).GetName(),f\" \\t\",s2.GetFeature(0).GetFieldAsString(s2.GetFeature(0).GetFieldIndex(layerDefinition.GetFieldDefn(i).GetName())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CA_DA = parseGMLtoDF('lda_000b16g_e.gml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df_CA_DA.head(2))\n",
    "print(df_CA_DA.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layered parsing code\n",
    "\n",
    "From when I thought dataframe copying was the limiting step\n",
    "\n",
    "    # Read in the file\n",
    "    source = ogr.Open('lda_000b16g_e.gml')\n",
    "    layer = source.GetLayer(0) # there is only one Layer (the FeatureCollection)\n",
    "\n",
    "    # Get a list of field names to extract (see lda_000b16g_e.xsd)\n",
    "    #   Fields are in order (sequence) and none are required (minOccurs=0)\n",
    "    layerDefinition = layer.GetLayerDefn()\n",
    "    layerFields = [layerDefinition.GetFieldDefn(i).GetName() for i in range(layerDefinition.GetFieldCount())]\n",
    "\n",
    "    %%time\n",
    "    import ipypb # Lightweight progress bar, source copied from GitHub\n",
    "\n",
    "    # Initialize the output dataframe - REUSE FOR ALL OF CANADA\n",
    "    df_CA_DA = pd.DataFrame(columns=[*layerFields, 'Geometry', 'Latitude', 'Longitude', 'Area'])\n",
    "\n",
    "    # Only process a few entries\n",
    "    parse_limit = 5\n",
    "\n",
    "    # Setup for 2-step dataframe appending, should be ~O(n^(1+e)) instead of ~O(n^2)\n",
    "    n_features = layer.GetFeatureCount()\n",
    "    #max_n_merge = 1000\n",
    "    n_merge = n_features**0.5//1\n",
    "    #n_merge = min(n_features**0.5//1,max_n_merge) # How many rows to process before a merge\n",
    "    df_tmp_collect = pd.DataFrame(columns=df_CA_DA.columns)\n",
    "\n",
    "    # Extract data from the features\n",
    "    layer.ResetReading()\n",
    "    for i in ipypb.track(range(layer.GetFeatureCount())):\n",
    "        # Get the feature to be processed\n",
    "        feature = layer.GetNextFeature()\n",
    "\n",
    "        # Copy all fields into an empty dataframe\n",
    "        df_tmp = pd.DataFrame(columns=df_CA_DA.columns)\n",
    "\n",
    "        # Copy all fields individually, in case some are missing\n",
    "        for i, fieldname in enumerate(layerFields):\n",
    "            if feature.IsFieldSet(i):\n",
    "                df_tmp.loc[0,fieldname] = feature.GetFieldAsString(fieldname)\n",
    "\n",
    "        # Get the latitude and longitude\n",
    "        inref = feature.GetGeometryRef().GetSpatialReference() # EPSG 3347\n",
    "        llref = osr.SpatialReference()\n",
    "        llref.ImportFromEPSG(4326) # coordinates for this geometry are latitude, longitude\n",
    "        lltransform = osr.CoordinateTransformation(inref, llref)\n",
    "        geom = feature.GetGeometryRef().Clone()\n",
    "        geom.Transform(lltransform)\n",
    "        centroid = geom.Centroid()\n",
    "        df_tmp['Latitude'] = centroid.GetX()\n",
    "        df_tmp['Longitude'] = centroid.GetY()\n",
    "        df_tmp['Geometry'] = geom.ExportToWkt()\n",
    "\n",
    "        # Get the area by converting to a locally-appropriate coordinate system\n",
    "        inref = feature.GetGeometryRef().GetSpatialReference()\n",
    "        arref = osr.SpatialReference()\n",
    "        arref.ImportFromEPSG(6931) # Lambert Cylindrical Equal Area 6933, U.S. National Atlas Equal Area Projection 2163, NSIDC EASE-Grid North 3408, WGS 84 / NSIDC EASE-Grid 2.0 North 6931, WGS 84 / NSIDC EASE-Grid 2.0 South 6932, WGS 84 / NSIDC EASE-Grid 2.0 Global/Temperate 6933\n",
    "        artransform = osr.CoordinateTransformation(inref, arref)\n",
    "        geom = feature.GetGeometryRef().Clone() # Fresh instance to prevent error accumulation from multiple transforms\n",
    "        geom.Transform(artransform)\n",
    "        df_tmp['Area'] = geom.Area()\n",
    "\n",
    "        # Add the feature dataframe to the output\n",
    "        #df_tmp_collect = df_tmp_collect.append(df_tmp, ignore_index=True)\n",
    "        df_tmp_collect = pd.concat([df_tmp_collect, df_tmp], ignore_index=True)\n",
    "\n",
    "        # Check if 2nd-step dataframe needs to be reset\n",
    "        if df_tmp_collect.shape[0] >= n_merge:\n",
    "            #df_CA_DA = df_CA_DA.append(df_tmp_collect, ignore_index=True)\n",
    "            df_CA_DA = pd.concat([df_CA_DA,df_tmp_collect], ignore_index=True)\n",
    "            df_tmp_collect = pd.DataFrame(columns=df_CA_DA.columns)\n",
    "\n",
    "        if not parse_limit is None:\n",
    "            parse_limit -= 1\n",
    "            if parse_limit<=0:\n",
    "                break\n",
    "    #df_CA_DA = df_CA_DA.append(df_tmp_collect, ignore_index=True)\n",
    "    df_CA_DA = pd.concat([df_CA_DA,df_tmp_collect], ignore_index=True)\n",
    "\n",
    "##### Timing Notes\n",
    "\n",
    "limit | n_merge | s/it | s total\n",
    "---|---|---|---\n",
    "500 | 1 | 0.13 | 1:06\n",
    "500 | 2 | 0.13 | 1:02\n",
    "500 | 10 | 0.12 | 1:01\n",
    "500 | 50 | 0.12 | 0:59.2\n",
    "500 | 237 | 0.11 | 0:57.5\n",
    "500 | 237 | 0.13 | 1:03\n",
    "1000 | 1 | 0.25 | 4:05\n",
    "1000 | 237 | 0.23 | 3:52 # maybe this was max instead of min?\n",
    "1000 | 1000 | 0.23 | 3:54\n",
    "1000 | 1000 | 0.24 | 3:55  # pd.concat instead of dataframe.append\n",
    "1000 | 237 | 0.22 | 3:38  # pd.concat instead of dataframe.append\n",
    "1000 | - | 0.27 | 4:25  # original, no 2-levels\n",
    "500  | - | 0.15 | 1:15  # original, no 2-levels\n",
    "500 | 237 | 0.07 | 0:34.3  # pd.concat instead of dataframe.append\n",
    "2000 | 237 | 0.07 | 2:13  # pd.concat instead of dataframe.append # Have not been resetting df_CA_DA!\n",
    "4000 | 237 | 0.06 | 4:15  # pd.concat instead of dataframe.append # Resetting df from here on, and concat too, and GetNextFeature instead of GetFeature(int) which may have been the bottleneck then...\n",
    "1000 | 237 | 0.06 | 1:01\n",
    "500 | 237 | 0.06 | 0:31.3\n",
    "1000 | 50 | 0.06 | 1:00\n",
    "1000 | 1 | 0.06 | 1:04\n",
    "1000 | 1 | 0.23 | 3:52  # Try once with GetFeature(i), yes, this was the culprit, it must parse anew each time.\n",
    "4000 | 1 | 0.09 | 5:41\n",
    "4000 | 5 | 0.06 | 4:11\n",
    "4000 | 20 | 0.06 | 3:41\n",
    "4000 | 50 | 0.06 | 3:54\n",
    "4000 | 50 | 0.06 | 3:57\n",
    "4000 | 100 | 0.06 | 3:58\n",
    "4000 | 175 | 0.06 | 4:03\n",
    "4000 | 237 | 0.06 | 4:08\n",
    "4000 | 500 | 0.06 | 4:18\n",
    "4000 | 500 | 0.06 | 3:50\n",
    "4000 | 1000 | 0.06 | 4:16\n",
    "4000 | 4000 | 0.07 | 4:58 # Oh, also hadn't reset the counter... adding that before last 500 and 30, that's been introducing variability too\n",
    "4000 | 1 | 0.06 | 4:16\n",
    "4000 | 30 | 0.06 | 4:17, 4:09 # Might change when tabs are changed (first with changing, second staying on same tab)\n",
    "4000 | 60 | 0.06 | 3:53, 4:03\n",
    "4000 | 200 | 0.06 | 3:56\n",
    "4000 | 500 | 0.06 | 3:44\n",
    "4000 | 1000 | 0.06 | 3:55\n",
    "4000 | 4000 | 0.06 | 4:03\n",
    "4000 | - | 0.06 | 4:04 # original, no 2-levels, with GetNextFeature\n",
    "\n",
    "Gain is marginal with the double layering... might be important for very large sets, but at 4000 features results in at most a 10% speedup.  Note that geopandas speedup is ~4,600%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting an appropriate EPSG projection\n",
    "\n",
    "Using UTM projections (of which there are 30, each subtending 6 degrees of longitude) was a commonly proposed method for projection for area calculation.  I went with the simpler EPSG-6931/2/3, which is easier due to one zone encompassing all of Canada (though with unknown accuracy tradeoff).  This approach is included just for reference.\n",
    "\n",
    "    srcSR = osr.SpatialReference()\n",
    "    srcSR.ImportFromEPSG(4326) # WGS84 Geographic\n",
    "    destSR   = osr.SpatialReference()\n",
    "\n",
    "    lyr.ResetReading()\n",
    "    for feat in lyr:\n",
    "        geom = feat.GetGeometryRef()\n",
    "        if not geom.IsEmpty():                 # make sure the geometry isn't empty\n",
    "            geom.AssignSpatialReference(srcSR) # you only need to do this if the shapefile isn't set or is set wrong\n",
    "            env = geom.GetEnvelope()           # get the Xmin, Ymin, Xmax, Ymax bounds\n",
    "            CentX = ( env[0] + env[2] ) / 2    # calculate the centre X of the whole geometry\n",
    "            Zone  = int((CentX + 180)/6) + 1   # see http://gis.stackexchange.com/questions/13291/computing-utm-zone-from-lat-long-point\n",
    "            EPSG  = 32600 + Zone               # get the EPSG code from the zone and the constant 32600 (all WGS84 UTM North start with 326)\n",
    "            destSR.ImportFromEPSG(EPSG)        # create the 'to' spatial reference\n",
    "            geom.TransformTo(destSR)           # project the geometry\n",
    "            print geom.GetArea()               # get the area in square metres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original df_loc (Canada) Description\n",
    "\n",
    "[Statistics Canada](https://www.statcan.gc.ca/eng/start) provides several geographic divisions for reporting of census statistics:\n",
    "\n",
    "Dissemination Area: DAs are small, relatively stable geographic unit composed of one or more adjacent dissemination blocks with an average population of 400 to 700 persons based on data from the previous Census of Population Program. It is the smallest standard geographic area for which all census data are disseminated.\n",
    "\n",
    "Aggregate Dissemination Area: ADAs cover the entire country and, where possible, have a population count between 5,000 and 15,000 people, and respect provincial, territorial, census division (CD), census metropolitan area (CMA) and census agglomeration (CA) with census tract (CT) boundaries in effect for the 2016 Census.\n",
    "\n",
    "Census Tract: CTs are small, relatively stable geographic areas that usually have a population of less than 10,000 persons, based on data from the previous Census of Population Program. They are located in census metropolitan areas and in census agglomerations that had a core population of 50,000 or more in the previous census.\n",
    "\n",
    "Census Metropolitan Area/ Census Agglomeration: A CMA or CA is formed by one or more adjacent municipalities centred on a population centre (known as the core). A CMA must have a total population of at least 100,000 of which 50,000 or more must live in the core based on adjusted data from the previous Census of Population Program. A CA must have a core population of at least 10,000 also based on data from the previous Census of Population Program. To be included in the CMA or CA, other adjacent municipalities must have a high degree of integration with the core, as measured by commuting flows derived from data on place of work from the previous Census Program.\n",
    "\n",
    "Census Subdivision: CSD is the general term for municipalities (as determined by provincial/territorial legislation) or areas treated as municipal equivalents for statistical purposes (e.g., Indian reserves, Indian settlements and unorganized territories).\n",
    "\n",
    "Census Consolidated Subdivision:A CCS is a group of adjacent census subdivisions within the same census division. Generally, the smaller, more densely-populated census subdivisions (towns, villages, etc.) are combined with the surrounding, larger, more rural census subdivision, in order to create a geographic level between the census subdivision and the census division.\n",
    "\n",
    "Census Division: CDs are a group of neighbouring municipalities joined together for the purposes of regional planning and managing common services (such as police or ambulance services).  Census divisions are intermediate geographic areas between the province/territory level and the municipality (census subdivision).\n",
    "\n",
    "Economic Region: An ER is a grouping of complete census divisions (CDs), with one exception in Ontario, created as a standard geographic unit for analysis of regional economic activity.\n",
    "\n",
    "Note: CSDNAME = 'Toronto' gives the same area as the FSAs, which makes sense as this is probably the definition of 'Toronto' \n",
    "\n",
    "The meaning of prefixes for NAME (N), UID (U), PUID (PU), TYPE (T), and CODE (C):\n",
    "* DA U Dissemination Area unique identifier (composed of the 2-digit province/territory unique identifier followed by the 2-digit census division code and the 4-digit dissemination area code)\n",
    "* PR U,N Province or territory\n",
    "* CD U,N,T Census Division\n",
    "* CCS U,N Census Consolidated Subdivision\n",
    "* CSD U,N,T Census Subdivision\n",
    "* ER U,N Economic Region\n",
    "* SAC T,C Statistical Area Classification: Part of are a component of a census metropolitan area, a census agglomeration, a census metropolitan influenced zone or the territories?\n",
    "* CMA U,PU,N,T Census Metropolitan Area or Census Agglomeration name, PU Uniquely identifies the provincial or territorial part of a census metropolitan area or census agglomeration (composed of the 2-digit province or territory unique identifier followed by the 3-digit census metropolitan area or census agglomeration unique identifier)\n",
    "* CT U,N Census Tract within census metropolitan area/census agglomeration\n",
    "* ADA U Aggregate dissemination area unique identifier\n",
    "\n",
    "We will use census Distribution Areas as proxies for neighborhoods for cities in Canada.  In previous work where the Forward Sortation Areas (first three characters of the postal code) were used as neighborhood proxies, the sizes of many areas were quite large (several kilometers across) and therefore are likely internally non-homogeneous from a features perspective at the walking-distance (500 m) length scale.  To convert to neighborhood names we can look up the associated census tract as seen on [this](https://en.wikipedia.org/wiki/Demographics_of_Toronto_neighbourhoods) Wikipedia page.\n",
    "\n",
    "File lda_000b16g_e.gml was downloaded from the [Statistics Canada: Boundary Files](https://www12.statcan.gc.ca/census-recensement/2011/geo/bound-limit/bound-limit-2016-eng.cfm) site.\n",
    "\n",
    "Exploring the gml file and computing the area and centroid of the distribution areas can be done using the [geopandas module](https://geopandas.org/).  Geopandas builds upon [osgeo](https://gdal.org/python/index.html) which can also be used to explore and compute with the gml file, but in testing was 46 times faster due to vectorization of many calculations compared to a naive approach (see Appendix).\n",
    "\n",
    "Latitude and Longitude need to be obtained from a projection with those units, e.g. [EPSG-4326](https://epsg.io/4326).  Area can be calculated from an equal-area projection, e.g. [EPSG-6931](https://epsg.io/6931) (though a geodesic area calculation would be more accurate for larger regions, that would have to come from an additional package such as [proj](https://proj.org/), but since all regions here are small such the curvature of the earth is negligible, and altitude indtroduces an additional error likely comparable or larger to the curvature error, we will proceed with a simpler equal-area projection calculation).  The geometry is saved as text in the [Well-Known Text (WKT)](https://en.wikipedia.org/wiki/Well-known_text_representation_of_geometry) representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Read in the Canadian data file: Original\n",
    "    gdf = geopandas.read_file('lda_000b16g_e.gml')\n",
    "    gdf.rename_geometry('Geometry', inplace=True) # Default geometry column name is 'geometry'; changed for consistent capitalization of columns\n",
    "    gdf.set_geometry('Geometry') # Renaming is insufficient; this sets special variable gdf.geometry = gdf['Geometry']\n",
    "    gdf['Area'] = gdf['Geometry'].to_crs(epsg=6931).area\n",
    "    gdf['Centroid'] = gdf['Geometry'].centroid\n",
    "    gdf['Geometry'] = gdf['Geometry'].to_crs(epsg=4326)\n",
    "    gdf['Centroid'] = gdf['Centroid'].to_crs(epsg=4326) # Only the set geometry is converted with gdf.to_crs(); all other geometry-containing columns must be converted explicitly; here we convert all columns explicitly\n",
    "    gdf['Centroid Latitude'] = gdf['Centroid'].geometry.y\n",
    "    gdf['Centroid Longitude'] = gdf['Centroid'].geometry.x\n",
    "    gdf.drop(columns = 'Centroid', inplace=True) # Because WKT Point cannot be serialized to JSON, we drop the Centroid column and keep only its float components\n",
    "    gdf_CA_DA = gdf # Rename because we may be generating additional variables\n",
    "    gdf_CA_DA.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used - this naive choropleth displays only one map and doesn't allow much scale customization\n",
    "\n",
    "city_index = 1\n",
    "\n",
    "# create a plain world map\n",
    "city_map = folium.Map(location=cities[city_index]['centroid'], control_scale=True)\n",
    "city_map.fit_bounds(cities[city_index]['bounds'])\n",
    "\n",
    "folium.Choropleth(\n",
    "    geo_data=cities[city_index]['geojson'],\n",
    "    data=cities[city_index]['data'],\n",
    "    columns=['DAUID', 'Area'],\n",
    "    key_on='feature.properties.DAUID',\n",
    "    fill_color='YlOrRd', \n",
    "    fill_opacity=0.7, \n",
    "    line_opacity=0.2,\n",
    "    legend_name='Area [square meters]'\n",
    ").add_to(city_map)\n",
    "\n",
    "# display map\n",
    "city_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Not used - links choropleth to a layercontrol, but does not get the legend on the correct layer\n",
    "\n",
    "from branca.element import MacroElement\n",
    "\n",
    "from jinja2 import Template\n",
    "\n",
    "class BindColormap(MacroElement):\n",
    "    \"\"\"Binds a colormap to a given layer.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    colormap : branca.colormap.ColorMap\n",
    "        The colormap to bind.\n",
    "    \"\"\"\n",
    "    def __init__(self, layer, colormap):\n",
    "        super(BindColormap, self).__init__()\n",
    "        self.layer = layer\n",
    "        self.colormap = colormap\n",
    "        self._template = Template(u\"\"\"\n",
    "        {% macro script(this, kwargs) %}\n",
    "            {{this.colormap.get_name()}}.svg[0][0].style.display = 'block';\n",
    "            {{this._parent.get_name()}}.on('overlayadd', function (eventLayer) {\n",
    "                if (eventLayer.layer == {{this.layer.get_name()}}) {\n",
    "                    {{this.colormap.get_name()}}.svg[0][0].style.display = 'block';\n",
    "                }});\n",
    "            {{this._parent.get_name()}}.on('overlayremove', function (eventLayer) {\n",
    "                if (eventLayer.layer == {{this.layer.get_name()}}) {\n",
    "                    {{this.colormap.get_name()}}.svg[0][0].style.display = 'none';\n",
    "                }});\n",
    "        {% endmacro %}\n",
    "        \"\"\")  # noqa\n",
    "        \n",
    "# https://gitter.im/python-visualization/folium?at=5a36090a03838b2f2a04649d\n",
    "print('Area Selection Criterion: CSDNAME contains the city name')\n",
    "import branca.element as bre\n",
    "from branca.colormap import LinearColormap\n",
    "\n",
    "f = bre.Figure()\n",
    "sf = [[],[],[]]\n",
    "city_map = [[],[],[]]\n",
    "colormap = [[],[],[]]\n",
    "cp = [[],[],[]]\n",
    "for i, city in enumerate(cities):\n",
    "    sf[i] = f.add_subplot(1,len(cities),1+i)\n",
    "    city_map[i] = folium.Map(location=city['centroid'], control_scale=True)\n",
    "    sf[i].add_child(city_map[i])\n",
    "    \n",
    "    title_html = '''<h3 align=\"center\" style=\"font-size:16px;charset=utf-8\"><b>{}</b></h3>'''.format(city['name'])\n",
    "    city_map[i].get_root().html.add_child(folium.Element(title_html))\n",
    "    city_map[i].fit_bounds(city['bounds'])\n",
    "    cp[i] = folium.Choropleth(geo_data=city['geojson'],\n",
    "                                data=city['data'],\n",
    "                                columns=['DAUID', 'Area'],\n",
    "                                key_on='feature.properties.DAUID',\n",
    "                                fill_color='YlOrRd',\n",
    "                                fill_opacity=0.7, \n",
    "                                line_opacity=0.2\n",
    "                               )\n",
    "    city_map[i].add_child(cp[i])\n",
    "    city_map[i].add_child(folium.map.LayerControl())\n",
    "    city_map[i].add_child(BindColormap(cp[i],cp[i].color_scale))\n",
    "\n",
    "    city['map'] = city_map[i]\n",
    "\n",
    "display(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original FSA-DA overlap calculator\n",
    "\n",
    "This code contains alternate gdf massaging options, along with some comments recording the effects of massaging on execution time.  This naive code ended up executing buffering on all DA geometries each time an intersection calculation failed, which was quite often, and led to very long computation times.  The code retained in the main section is a cleaned up version of this final code which computes buffering once at the beginning of execution, saving a lot of time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gdf1 = gdf_CA_FSA.copy(deep=True)\n",
    "keyfield1 = 'CFSAUID'\n",
    "gdf2 = gdf_CA_DA.copy(deep=True)\n",
    "keyfield2 = 'DAUID'\n",
    "\n",
    "gdf_union = geopandas.GeoDataFrame()\n",
    "times = []\n",
    "areas = []\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "from shapely.geometry import shape, mapping\n",
    "'''\n",
    "def _set_precision(coords, precision):\n",
    "    result = []\n",
    "    try:\n",
    "        return round(coords, int(precision))\n",
    "    except TypeError:\n",
    "        for coord in coords:\n",
    "            result.append(_set_precision(coord, precision))\n",
    "    return result\n",
    "\n",
    "gdf1['Geometry'] = [shapely.ops.unary_union(g) for g in gdf1['Geometry']]\n",
    "geo1 = [mapping(g) for g in gdf1['Geometry']]\n",
    "for g in geo1:\n",
    "    g['coordinates'] = _set_precision(g['coordinates'],8)\n",
    "gdf1['Geometry'] = [shape(g) for g in geo1]'''\n",
    "\n",
    "gdf1['Geometry'] = gdf1['Geometry'].buffer(0)\n",
    "#gdf1['Geometry'] = [shapely.geometry.MultiPolygon(shapely.ops.polygonize(shapely.ops.unary_union(g))) for g in gdf1['Geometry']]\n",
    "#gdf1['Geometry'] = [shapely.ops.unary_union(g) for g in gdf1['Geometry'].buffer(0)]\n",
    "print(f'Polygon conversion for {keyfield1} completed in {time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))}')\n",
    "start_time = time.time()\n",
    "'''\n",
    "gdf2['Geometry'] = [shapely.ops.unary_union(g) for g in gdf2['Geometry']]\n",
    "geo2 = [mapping(g) for g in gdf2['Geometry']]\n",
    "for g in geo2:\n",
    "    g['coordinates'] = _set_precision(g['coordinates'],8)\n",
    "gdf2['Geometry'] = [shape(g) for g in geo2]'''\n",
    "\n",
    "gdf2['Geometry'] = gdf2['Geometry'].buffer(0)\n",
    "#gdf2['Geometry'] = [shapely.geometry.MultiPolygon(shapely.ops.polygonize(shapely.ops.unary_union(g))) for g in gdf2['Geometry']]\n",
    "#gdf2['Geometry'] = [shapely.ops.unary_union(g) for g in gdf2['Geometry'].buffer(0)]\n",
    "print(f'Polygon conversion for {keyfield2} completed in {time.strftime(\"%H:%M:%S\", time.gmtime(time.time()-start_time))}')\n",
    "\n",
    "# Using unary_union only: [shapely.ops.unary_union(g) for g in gdf1['Geometry']]\n",
    "# Processed row 1001/56589, 1 overlap found in 0.3 sec, 1001 overlaps total in 00:07:45, total exceptions 1\n",
    "# Processed row 2001/56589, 1 overlap found in 0.3 sec, 2001 overlaps total in 00:16:04, total exceptions 3\n",
    "# Processed row 3001/56589, 1 overlap found in 0.3 sec, 3001 overlaps total in 00:21:41, total exceptions 3\n",
    "# Processed row 4001/56589, 1 overlap found in 0.3 sec, 4001 overlaps total in 00:27:00, total exceptions 3\n",
    "# Processed row 5001/56589, 1 overlap found in 0.4 sec, 5001 overlaps total in 00:43:14, total exceptions 15\n",
    "# Processed row 6001/56589, 1 overlap found in 75.4 sec, 6001 overlaps total in 01:25:24, total exceptions 52\n",
    "\n",
    "# Using polygonize: [shapely.geometry.MultiPolygon(shapely.ops.polygonize(shapely.ops.unary_union(g))) for g in gdf1['Geometry']]\n",
    "# Polygon conversion for CFSAUID completed in 00:02:07\n",
    "# Polygon conversion for DAUID completed in 00:01:56\n",
    "# Processed row 1001/56589, 1 overlap found in 0.3 sec, 1001 overlaps total in 00:28:54, total exceptions 22\n",
    "# Processed row 2001/56589, 1 overlap found in 0.3 sec, 2001 overlaps total in 00:44:38, total exceptions 32\n",
    "\n",
    "# Using _set_precision: https://gis.stackexchange.com/questions/188622/rounding-all-coordinates-in-shapely/276512\n",
    "# Polygon conversion for CFSAUID completed in 00:00:28\n",
    "# Polygon conversion for DAUID completed in 00:01:21\n",
    "# Exception at 308, 414, 422,...\n",
    "# Adding unary_union afterward:\n",
    "# Polygon conversion for CFSAUID completed in 00:01:26\n",
    "# Polygon conversion for DAUID completed in 00:01:49\n",
    "# Exceptions at 414, 532, 890, \n",
    "# Processed row 1001/56589, 1 overlap found in 0.4 sec, 1001 overlaps total in 00:07:35, total exceptions 3\n",
    "# Processed row 2001/56589, 1 overlap found in 0.2 sec, 2001 overlaps total in 00:14:49, total exceptions 6\n",
    "# Switching order so unary union is first:\n",
    "# Exceptions at 308, 414, 532, 567, 648...\n",
    "# Processed row 1001/56589, 1 overlap found in 0.3 sec, 1001 overlaps total in 00:22:10, total exceptions 23\n",
    "\n",
    "# Using buffered geometry only: gdf1['Geometry'] = gdf1['Geometry'].buffer(0)\n",
    "# Processed row 1001/56589, 1 overlap found in 0.2 sec, 1001 overlaps total in 00:05:11, total exceptions 0\n",
    "# Processed row 2001/56589, 1 overlap found in 0.2 sec, 2001 overlaps total in 00:10:10, total exceptions 0\n",
    "\n",
    "\n",
    "# gdf11 = gdf_CA_FSA.copy(deep=True)\n",
    "# gdf22 = gdf_CA_DA.copy(deep=True)\n",
    "# ratio1 = gdf11['Geometry'].area/gdf1['Geometry'].area\n",
    "# ratio1.unique()\n",
    "#  array([1.])\n",
    "\n",
    "exceptioncount = 0\n",
    "start_time = time.time()\n",
    "for i in range(gdf2.shape[0]):\n",
    "    loop_start = time.time()\n",
    "    # gdf_tmp = geopandas.overlay(gdf1[['Geometry']], gdf2[['Geometry']].iloc[[i],:], how='intersection') # This takes about 50 seconds per execution\n",
    "    try:\n",
    "        gdf_tmp = gdf1['Geometry'].intersection(gdf2['Geometry'].iloc[i]) # This takes about 0.3 seconds per execution\n",
    "    except (shapely.errors.TopologicalError): # This sometimes occurs e.g. index 421/422 calling a self-intersection; not sure what to make of it\n",
    "        print(f'Handling exception at index {i}')\n",
    "        exceptioncount += 1\n",
    "        gdf_tmp = gdf1['Geometry'].buffer(0).intersection(gdf2['Geometry'].iloc[i].buffer(0)) # This takes a long time again, e.g. 52 sec for row 423, but better than failure!\n",
    "        # Actually, these errors are discussed online, e.g. https://github.com/Toblerity/Shapely/issues/599 and https://stackoverflow.com/questions/35110632/splitting-self-intersecting-polygon-only-returned-one-polygon-in-shapely\n",
    "        # The errors arise when geometry lines cross when not at a node\n",
    "        # The buffer(0) trick can actually cause dropping of polygons, hence we use it only as a fallback\n",
    "        # For example, there were ~29 exceptions in the first 5000/56589 rows of gdf1, roughly doubling the expected computation time from 5 to 10 hours\n",
    "        # As a solution we perform the unary union on both input datasets before the looped intersection; this takes only ~2 minutes and prevents those errors which double the time\n",
    "    areas.append(gdf_tmp.area)\n",
    "    ind = np.argmax(areas[-1]) # The FSA boundaries respect DA boundaries according to the , so there is just one FSA associated with each DA, which should have significantly larger area than any other.\n",
    "    gdf_tmp = geopandas.GeoSeries(gdf_tmp.iloc[ind],crs=gdf1['Geometry'].crs)\n",
    "    gdf_tmp = geopandas.GeoDataFrame(geometry=gdf_tmp,crs=gdf_tmp.crs)\n",
    "    gdf_tmp[keyfield1] = gdf1[keyfield1].iloc[ind]\n",
    "    gdf_tmp[keyfield2] = gdf2[keyfield2].iloc[i]\n",
    "    gdf_union = gdf_union.append(gdf_tmp,ignore_index=True)\n",
    "    loop_end = time.time()\n",
    "    times.append(loop_end-loop_start)\n",
    "    if i%1000==0 or i+1==gdf2.shape[0]:\n",
    "        print(f'Processed row {i+1}/{gdf2.shape[0]}, {gdf_tmp.shape[0]} overlap found in {loop_end-loop_start:.1f} sec, {gdf_union.shape[0]} overlaps total in {time.strftime(\"%H:%M:%S\", time.gmtime(loop_end-start_time))}, total exceptions {exceptioncount}')\n",
    "print('Overlap processing complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Original extendBounds Function and Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extendBounds(bounds,method='nearestLeadingDigit',scale=10):\n",
    "    '''Extend bounds (low, high) to give round numbers for scalebars\n",
    "    \n",
    "    The low bound is decreased and the high bound is increased according to\n",
    "        method to the first number satisfying the method conditions.\n",
    "    Returns a new bound which includes the old bounds in its entirety\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bounds: (low_bound, high_bound) list or tuple\n",
    "    method: str describing the extension method\n",
    "        'nearestLeadingDigit': Bounds are nearest numbers with leading digit followed by zeros\n",
    "        'nearestPower': Bounds are nearest powers of scale (scale must be > 1).  For negative numbers, the sign and direction are reversed, the extension performed, then the sign of the result is reversed back.\n",
    "        'nearestMultiple': Bounds are nearest multiple of scale (scale must be > 0)\n",
    "        'round': Bounds are rounded\n",
    "    scale: numeric as described in method options\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    2-element tuple of extended bounds e.g. (newlow,newhigh)\n",
    "    '''\n",
    "    if bounds[0]>bounds[1]:\n",
    "        print('bounds must be ordered from least to greatest')\n",
    "        return None    \n",
    "    if method=='nearestLeadingDigit':\n",
    "        iszero = np.array(bounds)==0\n",
    "        isnegative = np.array(bounds) < 0\n",
    "        offsets = [1 if isnegative[0] else 0, 0 if isnegative[1] else 1]\n",
    "        power = [0 if z else np.floor(np.log10(abs(b))) for b, z in zip(bounds, iszero)]\n",
    "        firstdigit = [abs(b)//np.power(10,p) for b, p in zip(bounds, power)]\n",
    "        exceeds = [abs(b)>f*np.power(10,p) for b, f, p in zip(bounds, firstdigit, power)]\n",
    "        newbounds = [abs(b) if not t else (f+o)*np.power(10,p) for b, t, n, f, o, p in zip(bounds, exceeds, isnegative, firstdigit, offsets, power)]\n",
    "        newbounds = [-n if t else n for n, t in zip(newbounds,isnegative)]\n",
    "    elif method=='nearestPower':\n",
    "        try:\n",
    "            scale = float(scale)\n",
    "            if scale<=1:\n",
    "                print('scale should be greater than 1')\n",
    "                return None\n",
    "        except ValueError:\n",
    "            print('scale should be a number greater than 1')\n",
    "            return None\n",
    "        isnegative = np.array(bounds) < 0\n",
    "        roundfuns = [np.ceil if isnegative[0] else np.floor, np.floor if isnegative[1] else np.ceil]\n",
    "        newbounds = [0 if b==0 else np.power(scale, r(np.log10(abs(b))/np.log10(scale))) for b, r in zip(bounds,roundfuns)]\n",
    "        newbounds = [-n if t else n for n, t in zip(newbounds,isnegative)]\n",
    "    elif method=='nearestMultiple':\n",
    "        try:\n",
    "            scale = float(scale)\n",
    "            if scale<=0:\n",
    "                print('scale should be greater than 0')\n",
    "                return None\n",
    "        except ValueError:\n",
    "            print('scale should be a number greater than 0')\n",
    "            return None\n",
    "        newbounds = [scale*(np.floor(bounds[0]/scale)), scale*(np.ceil(bounds[1]/scale))]\n",
    "    elif method=='round':\n",
    "        newbounds = [np.floor(bounds[0]), np.ceil(bounds[1])]\n",
    "    else:\n",
    "        print('Invalid method, see help(extendBounds)')\n",
    "        return None\n",
    "    return newbounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing invalid method\")\n",
    "print(\"  Expect errors:\")\n",
    "print(f\"{extendBounds([11,130],'invalid')}\")\n",
    "print()\n",
    "\n",
    "print(\"Testing method 'nearestLeadingDigit'\")\n",
    "print(\"  Expect errors:\")\n",
    "print(f\"{extendBounds([9,-930],'nearestLeadingDigit')}\")\n",
    "print(f\"{extendBounds([-9,-930],'nearestLeadingDigit')}\")\n",
    "print(\"  Expect success:\")\n",
    "print(f\"{extendBounds([11,130],'nearestLeadingDigit')}\",)\n",
    "print(f\"{extendBounds([11,130],'nearestLeadingDigit',-1)}\")\n",
    "print(f\"{extendBounds([9,930],'nearestLeadingDigit')}\")\n",
    "print(f\"{extendBounds([-9,930],'nearestLeadingDigit')}\")\n",
    "print(f\"{extendBounds([-990,-930],'nearestLeadingDigit')}\")\n",
    "print(f\"{extendBounds([-990,0.05],'nearestLeadingDigit')}\")\n",
    "print(f\"{extendBounds([0,0.052],'nearestLeadingDigit')}\")\n",
    "print()\n",
    "\n",
    "print(\"Testing method 'nearestPower'\")\n",
    "print(\"  Expect errors:\")\n",
    "print(f\"{extendBounds([11,130],'nearestPower',-2)}\")\n",
    "print(f\"{extendBounds([11,130],'nearestPower',0)}\")\n",
    "print(f\"{extendBounds([11,130],'nearestPower',1)}\")\n",
    "print(f\"{extendBounds([-11,-130],'nearestPower',10)}\")\n",
    "print(\"  Expect success:\")\n",
    "print(f\"{extendBounds([11,130],'nearestPower')}\")\n",
    "print(f\"{extendBounds([10,100],'nearestPower')}\")\n",
    "print(f\"{extendBounds([11,130],'nearestPower',1.1)}\")\n",
    "print(f\"{extendBounds([11,130],'nearestPower',2)}\")\n",
    "print(f\"{extendBounds([11,130],'nearestPower',10)}\")\n",
    "print(f\"{extendBounds([11,130],'nearestPower',10.)}\")\n",
    "print(f\"{extendBounds([-11,130],'nearestPower',10)}\")\n",
    "print(f\"{extendBounds([-5100,-130],'nearestPower',10)}\")\n",
    "print(f\"{extendBounds([-.0101,-0.00042],'nearestPower',10)}\")\n",
    "print(f\"{extendBounds([0,0.00042],'nearestPower',10)}\")\n",
    "print()\n",
    "\n",
    "print(\"Testing method 'nearestMultiple'\")\n",
    "print(\"  Expect errors:\")\n",
    "print(f\"{extendBounds([11,132],'nearestMultiple',-2)}\")\n",
    "print(f\"{extendBounds([11,132],'nearestMultiple',0)}\")\n",
    "print(f\"{extendBounds([0,-10],'nearestMultiple',100)}\")\n",
    "print(\"  Expect success:\")\n",
    "print(f\"{extendBounds([11,132],'nearestMultiple')}\")\n",
    "print(f\"{extendBounds([10,130],'nearestMultiple')}\")\n",
    "print(f\"{extendBounds([11.55,132.55],'nearestMultiple',0.1)}\")\n",
    "print(f\"{extendBounds([11.55,132.55],'nearestMultiple',1)}\")\n",
    "print(f\"{extendBounds([11.55,132.55],'nearestMultiple',100)}\")\n",
    "print(f\"{extendBounds([-11,132],'nearestMultiple',10)}\")\n",
    "print(f\"{extendBounds([-1121,-132],'nearestMultiple',10)}\")\n",
    "print(f\"{extendBounds([-10,-10],'nearestMultiple',10)}\")\n",
    "print(f\"{extendBounds([-10,-10],'nearestMultiple',100)}\")\n",
    "print()\n",
    "\n",
    "print(\"Testing method 'round'\")\n",
    "print(\"  Expect errors:\")\n",
    "print(f\"{extendBounds([-11.1,-132.1],'round')}\")\n",
    "print(\"  Expect success:\")\n",
    "print(f\"{extendBounds([11.1,132.1],'round')}\")\n",
    "print(f\"{extendBounds([10,130],'round')}\")\n",
    "print(f\"{extendBounds([11.1,132.1],'round',-2)}\")\n",
    "print(f\"{extendBounds([-11.1,132.1],'round')}\")\n",
    "print(f\"{extendBounds([-1100.1,-132.1],'round')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Testing invalid method\n",
    "      Expect errors:\n",
    "    Invalid method, see help(extendBounds)\n",
    "    None\n",
    "\n",
    "    Testing method 'nearestLeadingDigit'\n",
    "      Expect errors:\n",
    "    bounds must be ordered from least to greatest\n",
    "    None\n",
    "    bounds must be ordered from least to greatest\n",
    "    None\n",
    "      Expect success:\n",
    "    [10.0, 200.0]\n",
    "    [10.0, 200.0]\n",
    "    [9, 1000.0]\n",
    "    [-9, 1000.0]\n",
    "    [-1000.0, -900.0]\n",
    "    [-1000.0, 0.05]\n",
    "    [0, 0.06]\n",
    "\n",
    "    Testing method 'nearestPower'\n",
    "      Expect errors:\n",
    "    scale should be greater than 1\n",
    "    None\n",
    "    scale should be greater than 1\n",
    "    None\n",
    "    scale should be greater than 1\n",
    "    None\n",
    "    bounds must be ordered from least to greatest\n",
    "    None\n",
    "      Expect success:\n",
    "    [10.0, 1000.0]\n",
    "    [10.0, 100.0]\n",
    "    [10.834705943388395, 142.04293198443193]\n",
    "    [8.0, 256.0]\n",
    "    [10.0, 1000.0]\n",
    "    [10.0, 1000.0]\n",
    "    [-100.0, 1000.0]\n",
    "    [-10000.0, -100.0]\n",
    "    [-0.1, -0.0001]\n",
    "    [0, 0.001]\n",
    "\n",
    "    Testing method 'nearestMultiple'\n",
    "      Expect errors:\n",
    "    scale should be greater than 0\n",
    "    None\n",
    "    scale should be greater than 0\n",
    "    None\n",
    "    bounds must be ordered from least to greatest\n",
    "    None\n",
    "      Expect success:\n",
    "    [10.0, 140.0]\n",
    "    [10.0, 130.0]\n",
    "    [11.5, 132.6]\n",
    "    [11.0, 133.0]\n",
    "    [0.0, 200.0]\n",
    "    [-20.0, 140.0]\n",
    "    [-1130.0, -130.0]\n",
    "    [-10.0, -10.0]\n",
    "    [-100.0, -0.0]\n",
    "\n",
    "    Testing method 'round'\n",
    "      Expect errors:\n",
    "    bounds must be ordered from least to greatest\n",
    "    None\n",
    "      Expect success:\n",
    "    [11.0, 133.0]\n",
    "    [10.0, 130.0]\n",
    "    [11.0, 133.0]\n",
    "    [-12.0, 133.0]\n",
    "    [-1101.0, -132.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCityBounds(cities, featurename):\n",
    "    '''Returns the global bounds of column featurename across all cities\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    cities: dict or list of dicts as defined above\n",
    "    featurename: str column name in cities[i]['gdf'] for which bounds will be obtained\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    (min, max) value across all cities\n",
    "    '''\n",
    "    if type(cities)==dict:\n",
    "        cities = [cities]\n",
    "    bounds = [[],[]]\n",
    "    for city in cities:\n",
    "        bounds[0].append(city['gdf'][featurename].min())\n",
    "        bounds[1].append(city['gdf'][featurename].max())\n",
    "    bounds[0] = min(bounds[0])\n",
    "    bounds[1] = max(bounds[1])\n",
    "    return bounds\n",
    "\n",
    "def extendBound(bound,direction='up',method='nearestLeadingDigit',scale=10):\n",
    "    '''Extend bound to next 'round' number\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    bound: float or float castable number or a list thereof\n",
    "    direction: {'up','down',nonzero number} or a list of these values indicating the direction to round in\n",
    "    method: str describing the extension method\n",
    "        'nearestLeadingDigit': Bound is nearest numbers with leading digit followed by zeros\n",
    "        'nearestPower': Bound is nearest integer power of scale (scale must be > 1).  For negative numbers, the sign and direction are reversed, the extension performed, then the sign of the result is reversed back.\n",
    "        'nearestMultiple': Bound is nearest multiple of scale (scale must be > 0)\n",
    "        'round': Bound is rounded using the default method\n",
    "    scale: numeric as described in method options or a list thereof\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    float: the extended bound\n",
    "    \n",
    "    Notes\n",
    "    -----\n",
    "    All inputs, if not single-valued, must be lists of length equal to input bound\n",
    "    TODO: extend so that method may also be a list\n",
    "    TODO: replace prints with raising errors\n",
    "    '''\n",
    "    import numpy as np\n",
    "    \n",
    "    # Check and adjust the length of inputs\n",
    "    unlist_bound = False\n",
    "    if not(type(bound) in {list,tuple,range}):\n",
    "        bound = [bound]\n",
    "        unlist_bound = True\n",
    "    acceptable_len = set((1,len(bound)))\n",
    "    if not(type(direction) in {list,tuple,range}):\n",
    "        direction = [direction]\n",
    "    if not(len(direction) in acceptable_len):\n",
    "        print('\"direction\" must have length 1 or length equal to the length of \"bound\"')\n",
    "        return None\n",
    "    if (type(method) in {str}):\n",
    "        method = [method]\n",
    "    if not(len(method) in acceptable_len):\n",
    "        print('\"method\" must have length 1 or length equal to the length of \"bound\"')\n",
    "        return None\n",
    "    if not(type(scale) in {list,tuple,range}):\n",
    "        scale = [scale]\n",
    "    if not(len(scale) in acceptable_len):\n",
    "        print('\"scale\" must have length 1 or length equal to the length of \"bound\"')\n",
    "        return None\n",
    "    if len(bound)>1:\n",
    "        if len(direction)==1: direction = [direction[0] for b in bound]\n",
    "        if len(scale)==1: scale = [scale[0] for b in bound]\n",
    "        \n",
    "    # If multiple methods are specified, recursively call this function for each method and reassemble results\n",
    "    if len(bound)>1 and len(method)>1:\n",
    "        ret = np.array([None for b in bound])\n",
    "        for m in list(set(method)):\n",
    "            ind = np.where(np.array(method)==m)\n",
    "            ret[ind] = extendBound(list(np.array(bound)[ind]),list(np.array(direction)[ind]),m,list(np.array(scale)[ind]))\n",
    "        return list(ret)\n",
    "    \n",
    "    # Convert direction to a logical array roundup\n",
    "    try:\n",
    "        roundup = [True if d=='up' else False if d=='down' else True if float(d)>0 else False if float(d)<0 else None for d in direction]\n",
    "    except:\n",
    "        print('direction must be \"up\", \"down\", or a non-negative number')\n",
    "        return None\n",
    "    if any([r==None for r in roundup]):\n",
    "        print('direction must be \"up\", \"down\", or a non-negative number')\n",
    "        return None\n",
    "    \n",
    "    # Cases for multiple methods handled above, return to string method\n",
    "    method = method[0]\n",
    "    \n",
    "    # Execute the conversions\n",
    "    if method=='nearestLeadingDigit':\n",
    "        iszero = np.array(bound)==0\n",
    "        isnegative = np.array(bound) < 0\n",
    "        offsets = np.logical_xor(roundup, isnegative)\n",
    "        power = [0 if z else np.floor(np.log10(abs(b))) for b, z in zip(bound, iszero)]\n",
    "        firstdigit = [abs(b)//np.power(10,p) for b, p in zip(bound, power)]\n",
    "        exceeds = [abs(b)>f*np.power(10,p) for b, f, p in zip(bound, firstdigit, power)]\n",
    "        newbound = [abs(b) if not t else (f+o)*np.power(10,p) for b, t, n, f, o, p in zip(bound, exceeds, isnegative, firstdigit, offsets, power)]\n",
    "        newbound = [-n if t else n for n, t in zip(newbound,isnegative)]\n",
    "    elif method=='nearestPower':\n",
    "        try:\n",
    "            scale = [float(s) for s in scale]\n",
    "            if any([s<=1 for s in scale]):\n",
    "                print('scale should be greater than 1')\n",
    "                return None\n",
    "        except ValueError:\n",
    "            print('scale should be a number or list of numbers greater than 1')\n",
    "            return None\n",
    "        isnegative = np.array(bound) < 0\n",
    "        offsets = np.logical_xor(roundup, isnegative)\n",
    "        roundfuns = [np.ceil if o else np.floor for o in offsets]\n",
    "        newbound = [0 if b==0 else np.power(s, r(np.log10(abs(b))/np.log10(s))) for b, r, s in zip(bound,roundfuns,scale)]\n",
    "        newbound = [-n if t else n for n, t in zip(newbound,isnegative)]\n",
    "    elif method=='nearestMultiple':\n",
    "        try:\n",
    "            scale = [float(s) for s in scale]\n",
    "            if any([s<=0 for s in scale]):\n",
    "                print('scale should be greater than 0')\n",
    "                return None\n",
    "        except ValueError:\n",
    "            print('scale should be a number or list of numbers greater than 0')\n",
    "            return None\n",
    "        roundfuns = [np.ceil if r else np.floor for r in roundup]\n",
    "        newbound = [s*(r(b/s)) for b, r, s in zip(bound,roundfuns,scale)]\n",
    "    elif method=='round':\n",
    "        roundfuns = [np.ceil if r else np.floor for r in roundup]\n",
    "        newbound = [f(b) for b, f in zip(bound, roundfuns)]\n",
    "    else:\n",
    "        print('Invalid method, see help(extendBound)')\n",
    "        return None\n",
    "    return newbound[0] if unlist_bound else newbound\n",
    "\n",
    "def extendBounds(bounds,method='nearestLeadingDigit',scale=10):\n",
    "    if bounds[0]>bounds[1]:\n",
    "        print('bounds must be ordered from least to greatest')\n",
    "        return None    \n",
    "    return extendBound(bounds,direction=['down','up'],method=method,scale=scale)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
